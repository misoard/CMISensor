{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":12641277,"sourceType":"datasetVersion","datasetId":7863206},{"sourceId":12641289,"sourceType":"datasetVersion","datasetId":7860371}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:26.550738Z","iopub.execute_input":"2025-08-01T13:55:26.551302Z","iopub.status.idle":"2025-08-01T13:55:26.560410Z","shell.execute_reply.started":"2025-08-01T13:55:26.551277Z","shell.execute_reply":"2025-08-01T13:55:26.559770Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_inference_server.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_gateway.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/__init__.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/templates.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/relay.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/__init__.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/__init__.py\n/kaggle/input/models/best_model_imu_only_fold_0_seed_39.pth\n/kaggle/input/models/best_model_fold_0_seed_39.pth\n/kaggle/input/models/best_model_imu_tof_thm_fold_0_seed_39.pth\n/kaggle/input/data-input/label_encoder.pkl\n/kaggle/input/data-input/split_ids.pkl\n/kaggle/input/data-input/all_parameters.pkl\n/kaggle/input/data-input/cols.pkl\n/kaggle/input/data-input/scaler.pkl\n/kaggle/input/data-input/train_torch_tensors_from_wrapper_left_corrected.pt\n/kaggle/input/data-input/train_torch_tensors_from_wrapper_left_corrected_without_TOF_correction.pt\n/kaggle/input/data-input/train_torch_tensors_from_wrapper_not_split.pt\n/kaggle/input/data-input/train_metadata.csv\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"import numpy as np\nimport warnings\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom IPython.display import display\nfrom scipy.spatial.transform import Rotation as R\nimport os, joblib\nimport torch\nimport torch.nn.functional as F\nfrom scipy.signal import find_peaks\nfrom scipy.signal import butter, filtfilt\nimport pickle\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score,  recall_score\nimport torch\nimport polars as pl\nfrom pathlib import Path\nimport inspect\nimport psutil\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\n\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# CONFIGURATION --\n# =============================================================================\n\nclass Config: \n    \"\"\"Central configuration class for training and data parameters\"\"\"\n\n    # Paths for Kaggle environment\n    TRAIN_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\"\n    TRAIN_DEMOGRAPHICS_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\"\n    TEST_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\"\n    TEST_DEMOGRAPHICS_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\"\n    EXPORT_DIR =  \"/kaggle/input/data-input\"\n    EXPORT_MODELS_PATH = \"/kaggle/input/models\" #\"/kaggle/working/models\"  \n    EXPORT_MODELS_PATH_OUTPUT = \"/kaggle/working/models\" #\"/kaggle/working/models\"  \n    os.makedirs(EXPORT_DIR, exist_ok=True)                                 \n    os.makedirs(EXPORT_MODELS_PATH, exist_ok=True)                                 \n    os.makedirs(EXPORT_MODELS_PATH_OUTPUT, exist_ok=True)     \n\n    # Training parameters\n    SEED = 42\n    N_FOLDS = 5\n    PERCENTILE = 95\n    PADDING = 127\n    \n    # Feature columns\n    ACC_COLS = ['acc_x', 'acc_y', 'acc_z']\n    ROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n    \n# Set reproducibility\nnp.random.seed(Config.SEED)\n\ndef check_gpu_availability():\n\n    import torch\n    if torch.cuda.is_available():\n        #print(\"MPS (Apple GPU) is available.\")\n        return  'cuda'#'cuda:0'\n    else:\n        #print(\"MPS not available. Using CPU.\")\n        return 'cpu'\n\n# Check GPU availability\nDEVICE = torch.device(check_gpu_availability())\nprint(DEVICE)\n\ndef clean_data(data_sequences, cols, prefix = 'both'):\n    \n    if prefix == 'both':\n        print(\"removing tof and thm missing data columns from sequences! Saving seq_id in a dic with cols to remove\")\n        tof_and_thm_cols = [col for col in cols if (col.startswith('thm') or col.startswith('tof')) ]\n    else: \n        print(f\"removing {prefix} missing data columns from sequences! Saving seq_id in a dic with cols to remove\")\n        tof_and_thm_cols = [col for col in cols if col.startswith(prefix) ]\n\n    tof_thm_nan_prefixes = {}\n    for sequence_id, sequence_data in data_sequences:\n        nan_cols = sequence_data[tof_and_thm_cols].columns[sequence_data[tof_and_thm_cols].isna().any()]\n        if nan_cols.any():\n            if (prefix == 'both' or prefix == 'tof'):\n                prefixes = set(col.rsplit(\"_\", 1)[0] for col in nan_cols if col.startswith('tof'))\n            else:\n                prefixes = set()\n            if (prefix == 'both' or prefix == 'thm'):\n                prefixes.update(set(col for col in nan_cols if col.startswith('thm')))\n\n            tof_thm_nan_prefixes[sequence_id] = prefixes\n            cols_to_drop = [col for col in sequence_data.columns if any(col.startswith(p) for p in prefixes)]\n            sequence_data = sequence_data.drop(columns=cols_to_drop)\n    print(f\"found {len(tof_thm_nan_prefixes)} sequences with missing data\")\n    return data_sequences, tof_thm_nan_prefixes\n\ndef handle_missing_values_quaternions(quaternion):\n    quat_clean = quaternion.copy()\n    \n    number_of_nan = quaternion.isna().sum(axis = 1)\n    rows_with_0_nan = number_of_nan == 0\n    rows_with_1_nan = number_of_nan == 1\n    rows_with_N_nan = number_of_nan > 1\n\n    ### normalize quaternions to 1 when no NaN has been detected \n    quat_values = quaternion.loc[rows_with_0_nan].values\n    norms = np.linalg.norm(quat_values, axis = 1)\n    normalized_quats = np.zeros_like(quat_values)\n    ## for non-zero norm, normalize to 1  \n    nonzero_norms = norms > 1e-6\n    normalized_quats[nonzero_norms] = quat_values[nonzero_norms] / norms[nonzero_norms, np.newaxis]\n    ## for zero-norm, normalize to the unit quaternion\n    normalized_quats[~nonzero_norms] = [1.0, 0.0, 0.0, 0.0]\n    ##update quaternion DataFrame\n    quat_clean.loc[rows_with_0_nan] = normalized_quats\n\n    ###handle 1 missing value \n    #use |w|² + |x|² + |y|² + |z|² = 1\n    if len(quaternion[rows_with_1_nan].index.tolist()) > 0:\n        nan_columns_per_row = quaternion[rows_with_1_nan].isna().idxmax(axis=1)\n        unnorm_quat = quaternion[rows_with_1_nan].pow(2).sum(axis =1, skipna = True)\n        vals = np.sqrt(np.maximum(0, 1 - unnorm_quat))\n        for row, col, val in zip(unnorm_quat.index, nan_columns_per_row, vals):\n            if row > 0:\n                if quat_clean.loc[row - 1, col] >= 0:\n                    quat_clean.loc[row, col] = val\n                else:\n                    quat_clean.loc[row, col] = -val\n            else:\n                next_row = row + 1\n                # Go forward until a non-NaN is found or reach the end\n                while next_row < len(quat_clean) and np.isnan(quat_clean.loc[next_row, col]):\n                    next_row += 1\n                if next_row == len(quat_clean):\n                    quat_clean.loc[rows_with_1_nan] = [0, 0, 0, 0]\n                    quat_clean.loc[rows_with_1_nan, 'rot_w'] = 1\n                    break\n                else:\n                    if quat_clean.loc[next_row, col] >= 0:\n                        quat_clean.loc[row, col] = val\n                    else:\n                        quat_clean.loc[row, col] = -val\n    quat_clean.loc[rows_with_N_nan] = [0, 0, 0, 0]\n    quat_clean.loc[rows_with_N_nan, 'rot_w'] = 1\n    return quat_clean\n\ndef check_missing_values_quaternion(data_sequences):\n    seq_id_quaternion_nan = []\n    check_norm_quaternion = []\n    for seq_id, data_sequence in data_sequences:\n        quaternion_cols = [col for col in data_sequence.columns if col.startswith('rot_')]\n        nan_quat_cols = data_sequence[quaternion_cols].columns[data_sequence[quaternion_cols].isna().any()]\n        normalize_quat = data_sequence[quaternion_cols].pow(2).sum(axis = 1).mean()\n        if nan_quat_cols.any():\n            #print(data_sequence[[col for col in data_sequence.columns if col.startswith('acc_')]])\n            seq_id_quaternion_nan.append(seq_id)\n        if (not nan_quat_cols.any()) and normalize_quat < 0.99:\n            check_norm_quaternion.append(seq_id)\n    print(f\"✓ number of seq_id with missing values in quaternion: {len(seq_id_quaternion_nan)}\")\n    print(f\"✓ number of unnormalized quaternions for complete quaternions: {len(check_norm_quaternion)}\")\n    return seq_id_quaternion_nan\n\n\ndef regularize_quaternions_per_sequence(data_sequence):\n    data_clean = data_sequence.copy()\n    quaternion_cols = [col for col in data_sequence.columns if col.startswith('rot_')]\n    nan_quat_cols = data_sequence[quaternion_cols].columns[data_sequence[quaternion_cols].isna().any()]\n    normalize_quat = data_sequence[quaternion_cols].pow(2).sum(axis = 1).mean()  \n    if nan_quat_cols.any():\n        data_clean[quaternion_cols] = handle_missing_values_quaternions(data_sequence[quaternion_cols])\n    if (not nan_quat_cols.any()) and normalize_quat < 0.99:\n        data_clean[quaternion_cols] = handle_missing_values_quaternions(data_sequence[quaternion_cols])\n\n    ### Check failed regularization\n    nan_quat_cols_clean = data_clean[quaternion_cols].columns[data_clean[quaternion_cols].isna().any()]\n    normalize_quat_clean = data_clean[quaternion_cols].pow(2).sum(axis = 1).mean() \n    if nan_quat_cols_clean.any():\n        print(\"!!NaN values have been detected after regularisation!!\")\n    if (not nan_quat_cols_clean.any()) and normalize_quat_clean < 0.99:\n        print(\"!!Not normalized quaternions have been detected after regularisation!!\")\n    return data_clean\n\n\n\ndef clean_and_check_quaternion(data):\n    data_clean = data.copy()\n    data_sequences = data_clean.groupby('sequence_id')\n    seq_id_quaternion_nan = check_missing_values_quaternion(data_sequences)\n    if len(seq_id_quaternion_nan) > 0:\n        for seq_id in seq_id_quaternion_nan:\n            data_sequence = data_sequences.get_group(seq_id)\n            idx = data_sequence.index  # Get the index of the group\n            quaternion_cols = [col for col in data_sequence.columns if col.startswith('rot_')]\n            # Apply quaternion cleaning function\n            data_clean.loc[idx, quaternion_cols] = handle_missing_values_quaternions(data_sequence[quaternion_cols])\n    ##Check quaternion\n        data_sequences = data_clean.groupby('sequence_id')\n        print(\"\")\n        print(\" --- missing values in quaternions have been handled ---\")\n        check_missing_values_quaternion(data_sequences)\n        print(\"\")\n    return data_clean\n\ndef compute_acceleration_features(sequence_data, demographics):\n    sequence_data_with_acc = sequence_data.copy()\n    correct_rot_order = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n    correct_acc_order = ['acc_x', 'acc_y', 'acc_z']\n    col_acc_world = ['acc_x_world', 'acc_y_world', 'acc_z_world']\n    col_linear_acc = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n    col_X_world = ['X_world_x', 'X_world_y', 'X_world_z']\n    col_Y_world = ['Y_world_x', 'Y_world_y', 'Y_world_z']\n    col_Z_world = ['Z_world_x', 'Z_world_y', 'Z_world_z']\n    remove_gravity = [0, 0, 9.81]\n    \n    data_rot = sequence_data[correct_rot_order]\n    data_acc = sequence_data[correct_acc_order]\n    sensor_x = np.zeros( data_acc.to_numpy().shape )\n    sensor_y = np.zeros( data_acc.to_numpy().shape )\n    sensor_z = np.zeros( data_acc.to_numpy().shape )\n    sensor_x[:, 0] = 1\n    sensor_y[:, 1] = 1\n    sensor_z[:, 2] = 1\n    data_rot_scipy = data_rot.to_numpy() \n\n    try:\n        r = R.from_quat(data_rot_scipy)\n        sequence_data_with_acc[col_acc_world] = pd.DataFrame(r.apply(data_acc.to_numpy()) - remove_gravity)\n        sequence_data_with_acc[col_X_world] = pd.DataFrame(r.apply(sensor_x))\n        sequence_data_with_acc[col_Y_world] = pd.DataFrame(r.apply(sensor_y))\n        sequence_data_with_acc[col_Z_world] = pd.DataFrame(r.apply(sensor_z))\n        \n        gravity_in_sensor = r.apply(remove_gravity, inverse=True)\n        acc_raw = sequence_data_with_acc[correct_acc_order].values\n        linear_acc = acc_raw - gravity_in_sensor\n        sequence_data_with_acc[col_linear_acc] = linear_acc\n\n    except ValueError:\n        print(\"Warning: world accelerations failed using device accelerations, replace by device acc data\")\n        sequence_data_with_acc[col_linear_acc] = sequence_data_with_acc[correct_acc_order]\n        sequence_data_with_acc[col_acc_world] = sequence_data_with_acc[correct_acc_order]\n        sequence_data_with_acc[col_X_world] = sequence_data_with_acc[correct_acc_order]\n        sequence_data_with_acc[col_Y_world] = sequence_data_with_acc[correct_acc_order]\n        sequence_data_with_acc[col_Z_world] = sequence_data_with_acc[correct_acc_order]\n\n    sequence_data_with_acc['acc_norm_world'] =sequence_data_with_acc[col_acc_world].apply(np.linalg.norm, axis=1)\n    sequence_data_with_acc['acc_norm'] =sequence_data_with_acc[correct_acc_order].apply(np.linalg.norm, axis=1)\n    sequence_data_with_acc['linear_acc_norm'] =sequence_data_with_acc[col_linear_acc].apply(np.linalg.norm, axis=1)\n    sequence_data_with_acc['acc_norm_jerk'] = sequence_data_with_acc['acc_norm'].diff().fillna(0)\n    sequence_data_with_acc['linear_acc_norm_jerk'] =  sequence_data_with_acc['linear_acc_norm'].diff().fillna(0)\n\n    subject = sequence_data['subject'].iloc[0]\n    handedness = demographics[demographics['subject'] == subject]['handedness'].iloc[0] ## (0): left, (1): right\n    if handedness == 0:\n        sequence_data_with_acc['acc_x'] = - sequence_data_with_acc['acc_x'] #+ (-0.8526133780336856 + 0.3518238644621146)\n        sequence_data_with_acc['linear_acc_x'] = - sequence_data_with_acc['linear_acc_x'] #+ (-0.8526133780336856 + 0.3518238644621146)\n\n    return sequence_data_with_acc\n\ndef compute_angular_features(sequence_data, demographics, time_delta = 10):\n    sequence_data_with_ang_vel = sequence_data.copy()\n    correct_rot_order = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n    quats = sequence_data[correct_rot_order].values\n\n    rotations = R.from_quat(quats)\n    rotvecs = rotations.as_rotvec()\n    sequence_data_with_ang_vel[['rotvec_x', 'rotvec_y', 'rotvec_z']] = rotvecs\n    sequence_data_with_ang_vel['angle_rad'] =  sequence_data_with_ang_vel[['rotvec_x', 'rotvec_y', 'rotvec_z']].apply(np.linalg.norm, axis=1)\n    rot_diff = sequence_data_with_ang_vel[['rotvec_x', 'rotvec_y', 'rotvec_z']].diff().fillna(0)\n    sequence_data_with_ang_vel['angular_speed'] = rot_diff.pow(2).sum(axis=1).pow(0.5)\n    sequence_data_with_ang_vel['rot_angle'] = 2 * np.arccos(sequence_data['rot_w'].clip(-1, 1))\n    sequence_data_with_ang_vel['rot_angle_vel'] = sequence_data_with_ang_vel['rot_angle'].diff().fillna(0)\n    \n    n_samples = quats.shape[0]\n    ang_vel = np.zeros( (n_samples, 3))\n    ang_dist = np.zeros(n_samples)\n\n    for i in range(n_samples - 1):\n        q1 = quats[i]\n        q2 = quats[i + 1]\n\n        if np.any(np.isnan(q1)) or np.any(np.isnan(q2)):\n            continue\n\n        try:\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n\n            # Relative rotation from q1 to q2\n            delta_r = r1.inv() * r2\n\n            # Angle of rotation (in radians)\n            ang_vel[i, : ] =  delta_r.as_rotvec()/time_delta\n            ang_dist[i] = np.linalg.norm(delta_r.as_rotvec())\n        except ValueError:\n            pass\n\n    sequence_data_with_ang_vel[['ang_vel_x', 'ang_vel_y', 'ang_vel_z']] = ang_vel\n    sequence_data_with_ang_vel['ang_dist'] = ang_dist\n\n    subject = sequence_data['subject'].iloc[0]\n    handedness = demographics[demographics['subject'] == subject]['handedness'].iloc[0] ## (0): left, (1): right\n    if handedness == 0:\n        sequence_data_with_ang_vel['rotvec_x'] = - sequence_data_with_ang_vel['rotvec_x'] #+ (-0.8526133780336856 + 0.3518238644621146)\n        sequence_data_with_ang_vel['ang_vel_y'] = - sequence_data_with_ang_vel['ang_vel_y'] #+ (-0.8526133780336856 + 0.3518238644621146)\n        sequence_data_with_ang_vel['ang_vel_z'] = - sequence_data_with_ang_vel['ang_vel_z'] #+ (-0.8526133780336856 + 0.3518238644621146)\n\n    return sequence_data_with_ang_vel\n\ndef fft_gesture(signal):\n    \"\"\"\n    Compute the normalized power in a band around a target frequency.\n\n    Parameters:\n    - signal: 1D array-like signal\n    - freq: frequency of interest (Hz)\n    - sampling_rate: sampling rate in Hz\n    - bandwidth_ratio: fraction of freq to define integration window (e.g., 0.05 for ±5%)\n\n    Returns:\n    - normalized_band_power: power in [freq ± bandwidth] / total power\n    \"\"\"\n    signal = np.asarray(signal)\n    n = len(signal)\n    #freqs = np.fft.rfftfreq(n, d=1./sampling_rate)\n    fft_vals = np.fft.rfft(signal)\n    power_spectrum = np.abs(fft_vals)**2 / n\n    return power_spectrum / np.sum(power_spectrum)\n\ndef compute_fft_features(sequence_data):\n    sequence_data_fft = sequence_data.copy()\n    fft_to_compute = [\n        'acc_x', 'acc_y', 'acc_z',\n        'linear_acc_x', 'linear_acc_y', 'linear_acc_z',\n        'rotvec_x', 'rotvec_y', 'rotvec_z',\n        'ang_vel_x', 'ang_vel_y', 'ang_vel_z',\n        'acc_norm', 'angle_rad'\n    ]\n    check_quat = ['rot_x', 'rot_y', 'rot_z']\n    phase_gesture = sequence_data['phase_adj'] == 1    \n    for feat in fft_to_compute:\n        signal = sequence_data.loc[phase_gesture, feat].to_numpy()\n        if sequence_data[check_quat].apply(np.linalg.norm, axis=1).mean() < 1e-6:\n            signal_fft_pad = np.zeros_like(sequence_data[feat])\n        else:\n            signal_fft = fft_gesture( (signal - np.mean(signal))/np.std(signal) )\n            signal_fft_pad =np.pad(signal_fft, (0, len(phase_gesture) - len(signal_fft)), 'constant')\n        sequence_data_fft[f'{feat}_FFT'] = signal_fft_pad\n    \n    return sequence_data_fft\n\ndef get_angles(time_series, world_coord = False):\n    theta, phi = [], []\n    acc_features = ['acc_norm', 'acc_x', 'acc_y', 'acc_z']\n    f_phi, f_theta = 'phi', 'theta'\n    if world_coord:\n        add_name = '_world'\n        acc_features = [f + add_name for f in acc_features]\n        f_phi, f_theta = f_phi + add_name, f_theta + add_name\n\n    #numpy_time_series = time_series[acc_features].to_numpy()\n    for a, ax, ay, az in zip(*time_series[acc_features].to_numpy().T):\n        # Avoid division by zero\n        # if a < 0:\n        #     print(a)\n        th = np.arccos(np.clip(az / (a + 1e-8), -1.0, 1.0))  # polar angle\n        ph = np.arctan2(ay, ax)  # azimuthal angle\n        theta.append(th)\n        phi.append(ph)\n    time_series[f_theta] = np.array(theta)\n    time_series[f_phi] = np.array(phi)\n    return time_series\n\ndef autocorr_frequency(signal, sampling_rate=1.0, min_lag=2, max_lag=None):\n    \"\"\"\n    Estimate the dominant frequency in a signal using autocorrelation.\n\n    Parameters:\n    - signal: list or np.array of values\n    - sampling_rate: Hz\n    - min_lag: minimum lag to consider (to skip lag 0 and noise)\n    - max_lag: optional max lag to consider\n\n    Returns:\n    - dominant_freq: float or None (in Hz)\n    \"\"\"\n    signal = np.array(signal)\n    if len(signal) < min_lag + 2:\n        return 0.\n\n    # Normalize and detrend\n    signal = signal - np.mean(signal)\n    autocorr = np.correlate(signal, signal, mode='full')\n    autocorr = autocorr[len(autocorr)//2:]  # Keep only non-negative lags\n    autocorr /= autocorr[0]  # Normalize\n\n    # Define lag range to search\n    if max_lag is None:\n        max_lag = len(signal) #// 2\n\n    search_range = autocorr[min_lag:max_lag]\n\n    # Find peaks in the autocorrelation\n    peaks, _ = find_peaks(search_range)\n\n    if len(peaks) < 2:\n        return 0.\n\n    first_peak_lag = (peaks[-1] - peaks[0])/(len(peaks)-1)  # adjust for sliced lag\n    period = first_peak_lag / sampling_rate\n    freq = 1.0 / period\n\n    return freq\n\ndef remove_frequency_component(signal, freq, sampling_rate, bandwidth=1.0, order=4):\n    \"\"\"\n    Remove a specific frequency component using a Butterworth band-stop filter.\n\n    Parameters:\n    - signal: np.array of the signal values\n    - freq: the target frequency to remove (Hz)\n    - sampling_rate: the sampling rate of the signal (Hz)\n    - bandwidth: the width of the stop band (Hz)\n    - order: filter order (higher = steeper filter)\n\n    Returns:\n    - filtered_signal: the signal with the frequency component removed\n    \"\"\"\n    bandwidth = 1 * freq\n    nyquist = 0.5 * sampling_rate\n    low = (freq - bandwidth / 2) / nyquist\n    high = (freq + bandwidth / 2) / nyquist\n\n    if low <= 0 or high >= 1:\n        # Invalid range – don't apply filtering\n        return signal.copy()\n\n    # Create band-stop filter\n    b, a = butter(order, [low, high], btype='bandstop')\n\n    # Calculate required padding length\n    padlen = 3 * max(len(a), len(b))\n\n    if len(signal) <= padlen:\n        # Too short for reliable filtering\n        b, a = butter(order, [low, high], btype='bandstop')\n        padlen = 3 * max(len(a), len(b))\n        filtered_signal = filtfilt(b, a, signal, padlen=min(padlen, len(signal) - 1))\n        # if len(signal) <= padlen:\n        #     print(\"short\")\n        #     return signal.copy()\n    else:\n        filtered_signal = filtfilt(b, a, signal)\n\n    return filtered_signal\n\n\ndef extract_freq_features(signal, fs=10):  # signal: [T, 3] for x,y,z IMU\n    features = []\n    for axis in range(signal.shape[1]):\n        f, Pxx = welch(signal[:, axis], fs=fs, nperseg=fs)\n        Pxx /= Pxx.sum()  # Normalize power spectrum\n\n        centroid = np.sum(f * Pxx)\n        entropy_val = entropy(Pxx)\n        rolloff = f[np.where(np.cumsum(Pxx) >= 0.85)[0][0]]\n        peak_freq = f[np.argmax(Pxx)]\n        flatness = np.exp(np.mean(np.log(Pxx + 1e-8))) / (np.mean(Pxx) + 1e-8)\n\n        features += [centroid, entropy_val, rolloff, peak_freq, flatness]\n\n    return features  # [5 features x 3 axes = 15 features]\n\ndef sliding_window_freq_features(data_sequence, fs=10, window_size=10, stride=10):\n    \"\"\"\n    data: (N, T, C) - batch of sequences\n    returns: (N, T_new, F_freq)\n    \"\"\"\n    names = ['centroid', 'entropy_val', 'rolloff', 'peak_freq', 'flatness']\n    data_sequence_with_FFT = data_sequence.copy()\n    signal = data_sequence[['acc_x', 'acc_y', 'acc_z']].to_numpy()\n    T, _ = signal.shape\n    features = []\n    for i in range(0, T - window_size + 1, stride):\n        window = signal[i:i+window_size, :]  # (N, w, C)\n        f_list = extract_freq_features(window, fs=fs)  # for each sequence\n        for j in range(window_size):\n            features.append(f_list)  # (T, F_freq)\n    print(np.array(features).shape)\n    data_sequence_with_FFT[names] = np.array(features)\n    # Stack over time: (T_new, N, F) → transpose → (N, T_new, F)\n    return data_sequence_with_FFT\n\n\ndef compute_theta_phi_features(sequence_data):\n    sequence_data_theta_phi = sequence_data.copy()\n        \n    sequence_data_theta_phi = get_angles(sequence_data_theta_phi)\n    sequence_data_theta_phi = get_angles(sequence_data_theta_phi, world_coord=True)\n\n    signal_phi = sequence_data_theta_phi['phi_world'].to_numpy()\n    \n    dym_zero_cross = np.zeros(len(signal_phi))\n    window_size = 10\n    for i in range(window_size, len(signal_phi)):\n        window_phi = signal_phi[i-window_size: i]\n        dym_zero_cross[i] = np.sum(np.diff(np.signbit(window_phi)).astype(int))\n\n    sequence_data_theta_phi['zero_crossings_phi_dyn'] = dym_zero_cross\n    return sequence_data_theta_phi\n\ndef compute_corr_and_svd_features(sequence_data):\n    sequence_data_with_corr_and_svd = sequence_data.copy()\n\n    svd_axis = [\n        ['acc_x', 'acc_y', 'acc_z'],\n        ['linear_acc_x', 'linear_acc_y', 'linear_acc_z'],\n        ['rotvec_x', 'rotvec_y', 'rotvec_z'],\n        ['ang_vel_x', 'ang_vel_y', 'ang_vel_z']\n    ]\n    corr_features = [\n        ('acc_x', 'acc_y'),\n        ('acc_x', 'acc_z'),\n        ('acc_y', 'acc_z'),\n        ('linear_acc_x', 'linear_acc_y'),\n        ('linear_acc_x', 'linear_acc_z'),\n        ('linear_acc_y', 'linear_acc_z'),\n        ('ang_vel_x', 'ang_vel_y'),\n        ('ang_vel_x', 'ang_vel_z'),\n        ('ang_vel_y', 'ang_vel_z'),\n        ('rotvec_x', 'rotvec_y'),\n        ('rotvec_x', 'rotvec_z'),\n        ('rotvec_y', 'rotvec_z'),\n        ('acc_norm', 'angle_rad'),\n        ('acc_x', 'rotvec_x'),\n        ('acc_x', 'rotvec_y'),\n        ('acc_x', 'rotvec_z'),\n        ('acc_y', 'rotvec_x'),\n        ('acc_y', 'rotvec_y'),\n        ('acc_y', 'rotvec_z'),\n        ('acc_z', 'rotvec_x'),\n        ('acc_z', 'rotvec_y'),\n        ('acc_z', 'rotvec_z'),\n        ('theta', 'phi'),\n        ('theta_world', 'phi_world')\n    ]\n\n    for main_axes in svd_axis:\n        #svd_features = [f + '_svd' for f in main_axes]\n        principal_axis_features = [f + '_contribution_main_axis' for f in main_axes]\n\n        name = '_'.join(main_axes[0].split('_')[:-1])\n        svd_ratio_features = [f'{name}_ratio_svd_{i}' for i in range(len(main_axes[1:]))]\n        svd_features = [f'{name}_svd_{i}' for i in range(len(main_axes))]\n\n        acc_vec = sequence_data[main_axes].to_numpy()\n\n        window_size = 10\n        sv =  np.zeros( (3, len(acc_vec)) )\n        sv_ratio = np.zeros( (2, len(acc_vec)) )\n        principal_axis = np.zeros( (3, len(acc_vec)) )\n        for i in range(window_size, len(acc_vec)):\n            window = acc_vec[i-window_size: i]\n            U, S, Vt = np.linalg.svd(window - window.mean(axis = 0))\n            principal_axis[:, i] =  Vt[0] ** 2\n            sv[:, i] = S\n            sv_ratio[0, i] = S[1]/S[0]\n            sv_ratio[1, i] = S[2]/S[0]\n\n        sequence_data_with_corr_and_svd[svd_features] = sv.T        \n        sequence_data_with_corr_and_svd[principal_axis_features] = principal_axis.T\n        sequence_data_with_corr_and_svd[svd_ratio_features] = sv_ratio.T \n\n    phase_transition = sequence_data['phase_adj'] == 0\n    phase_gesture = sequence_data['phase_adj'] == 1\n\n    #f_freq = [f for f in sequence_data.columns if ('acc' in f) or ('rotvec' in f) or ('angle_rad' in f) or ('phi' in f) or ('theta' in f) or ('ang_vel' in f)]\n    f_freq = [f for f in sequence_data.columns if any(substr in f for substr in ['acc', 'rotvec', 'angle_rad', 'phi', 'theta', 'angle_vel'])]\n\n    for f in f_freq:\n        f0_series = np.zeros(len(sequence_data))\n        f1_series = np.zeros(len(sequence_data))\n        ratio_freq = np.zeros(len(sequence_data))\n        if phase_gesture.sum() > 1:  \n            extracted_sig = sequence_data.loc[phase_gesture, f]\n            f0 = autocorr_frequency(extracted_sig, sampling_rate=10)\n            if f0 > 0:\n                residual = remove_frequency_component(extracted_sig, f0, sampling_rate=10)\n                f1 = autocorr_frequency(residual, sampling_rate=10)\n                ratio_freq[phase_gesture] = f1 / f0\n\n            else:\n                f1 = 0.\n                ratio_freq[phase_gesture] = 0.\n\n            f0_series[phase_gesture] = f0\n            f1_series[phase_gesture] = f1\n            \n        else:\n            f0_series[phase_gesture] = 0.\n            f1_series[phase_gesture] = 0.\n            ratio_freq[phase_gesture] = 0.\n\n        sequence_data_with_corr_and_svd[f'{f}_f0'] = f0_series\n        sequence_data_with_corr_and_svd[f'{f}_f1'] = f1_series\n        sequence_data_with_corr_and_svd[f'{f}_ratio_freqs'] = ratio_freq\n\n\n    for sig1, sig2 in corr_features:\n        # Initialize correlation series\n        corr_series = np.zeros(len(sequence_data))\n\n        if phase_transition.sum() > 1:  \n            corr_trans = sequence_data.loc[phase_transition, sig1].corr(sequence_data.loc[phase_transition, sig2])\n            corr_series[phase_transition] = corr_trans\n        else:\n            corr_series[phase_transition] = 0. \n\n        if phase_gesture.sum() > 1:\n            corr_gest = sequence_data.loc[phase_gesture, sig1].corr(sequence_data.loc[phase_gesture, sig2])\n            corr_series[phase_gesture] = corr_gest\n        else:\n            corr_series[phase_gesture] = 0.\n\n        # Save in your dataframe\n        sequence_data_with_corr_and_svd[f'{sig1}_{sig2}_corr'] = corr_series\n\n    return sequence_data_with_corr_and_svd\n\ndef add_gesture_phase(sequence_data):\n    sequence_data_phase = sequence_data.copy()\n    length_sequence = len(sequence_data)\n    idx_transition = int( 0.45 * length_sequence)\n    phase = np.zeros(length_sequence)\n    phase[idx_transition:] = 1.\n    sequence_data_phase['phase_adj'] = phase\n    return sequence_data_phase\n\ndef manage_tof(sequence_data, demographics):\n    sequence_data_tof = sequence_data.copy()\n    #tof_col = []\n    for i in range(1, 6):\n        pixel_cols = [f for f in sequence_data.columns if f'tof_{i}' in f]\n        tof_data = sequence_data[pixel_cols].replace(-1, np.nan)\n        sequence_data_tof[f'tof_{i}_mean'] = sequence_data[pixel_cols].mean(axis = 1)\n        sequence_data_tof[f'tof_{i}_std'] = sequence_data[pixel_cols].std(axis = 1)\n        sequence_data_tof[f'tof_{i}_min'] = tof_data.min(axis = 1)\n        sequence_data_tof[f'tof_{i}_max'] = tof_data.max(axis = 1)\n\n    subject = sequence_data['subject'].iloc[0]\n    handedness = demographics[demographics['subject'] == subject]['handedness'].iloc[0] ## (0): left, (1): right\n    if handedness == 2:\n        cols_tof_3 = [col for col in sequence_data.columns if 'tof_3' in col]\n        cols_thm_3 = [col for col in sequence_data.columns if 'thm_3' in col]\n        cols_tof_5 = [col for col in sequence_data.columns if 'tof_5' in col]\n        cols_thm_5 = [col for col in sequence_data.columns if 'thm_5' in col]\n        rename_dict = {}\n        # TOF3 <-> TOF5\n        for c3, c5 in zip(cols_tof_3, cols_tof_5):\n            rename_dict[c3] = c5\n            rename_dict[c5] = c3\n\n        # THM3 <-> THM5\n        for c3, c5 in zip(cols_thm_3, cols_thm_5):\n            rename_dict[c3] = c5\n            rename_dict[c5] = c3\n\n        sequence_data_tof.rename(columns=rename_dict, inplace=True)\n\n    return sequence_data_tof\n\n# def add_correlations_tof_imu(sequence_data):\n\ndef split_into_transition_and_gesture_phases(sequence_data, meta_cols):\n    sequence_data_split = sequence_data.copy()\n    df_transition = sequence_data[sequence_data['phase_adj'] == 0].drop(columns='phase_adj')\n    df_gesture = sequence_data[sequence_data['phase_adj'] == 1].drop(columns='phase_adj')\n\n    # Rename columns\n    df_transition = df_transition.add_suffix('_transition')\n    df_gesture = df_gesture.add_suffix('_gesture')\n\n    # Pad shorter DataFrame with NaNs to match the longer one\n    max_len = max(len(df_transition), len(df_gesture))\n\n    df_transition = df_transition.reset_index(drop=True).reindex(range(max_len))\n    df_gesture = df_gesture.reset_index(drop=True).reindex(range(max_len))\n\n    # Concatenate along columns\n    df_combined = pd.concat([df_transition, df_gesture], axis=1)\n    # Drop transition versions of meta columns\n    df_combined.drop(columns=[col + '_transition' for col in meta_cols], inplace=True)\n    # Rename gesture versions of meta columns back to original names\n    df_combined.rename(columns={col + '_gesture': col for col in meta_cols}, inplace=True)\n\n    sequence_data_split = df_combined.fillna(0)\n    return sequence_data_split\n\n\ndef wrapper_data( TRAIN = True, split = False):\n    if TRAIN:\n        train_df = pd.read_csv(Config.TRAIN_PATH)\n        train_demographics = pd.read_csv(Config.TRAIN_DEMOGRAPHICS_PATH)\n\n        label_encoder = LabelEncoder()\n        train_df['gesture_id'] = label_encoder.fit_transform(train_df['gesture'].astype(str))\n        joblib.dump(label_encoder, os.path.join(Config.EXPORT_DIR, \"label_encoder.pkl\"))\n\n        gesture_id_to_gestures = {idx: cl for idx, cl in enumerate(label_encoder.classes_)}\n\n        gesture_to_seq_ids = (\n            train_df.groupby('gesture_id')['sequence_id']\n            .unique()\n            .apply(list)\n            .to_dict()\n        )\n\n        seq_type_to_seq_ids = (\n            train_df.groupby('sequence_type')['sequence_id']\n            .unique()\n            .apply(list)\n            .to_dict()\n        )\n\n        train_sequence_subject = {\n            seq_id: sequence['subject'].iloc[0]\n            for seq_id, sequence in train_df.groupby('sequence_id')\n        }\n\n        train_sequence_ids = sorted(train_df['sequence_id'].unique())\n\n\n        train_cols = set(train_df.columns)\n\n\n        # Group by sequence_id for training data - need to include gesture column for labels\n        train_cols = train_cols + ['gesture_id'] if 'gesture_id' not in train_cols else train_cols\n\n        print(\"Handle quaternion missing values in the train dataset...\")\n        train_df_clean = clean_and_check_quaternion(train_df[train_cols])\n\n\n        train_sequences = train_df_clean.groupby('sequence_id')\n\n\n        split_ids = {\n            'classes': gesture_id_to_gestures,\n            'train': {\n                'train_sequence_ids': train_sequence_ids, ##List of all train ids\n                'train_sequence_subject': train_sequence_subject, ##List of all train subject\n                'gesture_to_seq_ids': gesture_to_seq_ids, ##dic by gesture\n                'seq_type_to_seq_ids': seq_type_to_seq_ids ##dic by sequence_type\n            },\n        }\n        # Save\n        with open(os.path.join(Config.EXPORT_DIR, 'split_ids.pkl'), 'wb') as f:\n            pickle.dump(split_ids, f)\n        \n\n        ### FEATURES ####\n        meta_cols = sorted(['gesture', 'gesture_id', 'sequence_type', 'behavior', 'orientation',\n                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'])\n        train_df_clean[meta_cols].to_csv( os.path.join(Config.EXPORT_DIR, 'train_metadata.csv' ))\n\n        features_cols = [c for c in train_cols if c not in meta_cols]\n        print(\"adding new features...\")\n        processed_sequences = []\n        for _, data_sequence in tqdm(train_sequences, desc=\"Processing Sequences\"):\n            data_sequence = data_sequence.reset_index(drop=True)\n            data_sequence = add_gesture_phase(data_sequence)\n            data_sequence = compute_acceleration_features(data_sequence, train_demographics)\n            data_sequence = compute_angular_features(data_sequence, train_demographics)\n            #data_sequence = compute_fft_features(data_sequence)\n            #data_sequence = compute_theta_phi_features(data_sequence) \n            #data_sequence = compute_corr_and_svd_features(data_sequence)\n            data_sequence = manage_tof(data_sequence, train_demographics)\n\n            if split:\n                data_sequence = split_into_transition_and_gesture_phases(data_sequence, meta_cols)\n\n            #print(data_sequence[['acc_x_transition', 'acc_x_gesture']])\n\n            processed_sequences.append(data_sequence)\n    \n        train_df_clean = pd.concat(processed_sequences).sort_index()\n\n        train_cols = train_df_clean.columns\n        #new_features = [c for c in cols if c not in features_cols and c not in meta_cols]\n        features_cols = [c for c in train_cols if c not in meta_cols]\n        imu_cols  = sorted([c for c in features_cols if not (c.startswith('thm_') or c.startswith('tof_'))])\n        tof_cols  = sorted([c for c in features_cols if c.startswith('tof_')])\n        thm_cols  = sorted([c for c in features_cols if c.startswith('thm_')])\n\n        fixed_order_features = np.concatenate( (imu_cols, thm_cols, tof_cols) )\n\n\n        print(f\"all features have been generated\")\n        # global scaler\n        #features_to_exclude = [f for f in fixed_order_features if ('svd' in f) or ('contribution_main_axis' in f) or ('f0' in f)]  # for example\n        features_to_exclude = [f for f in fixed_order_features if any(substr in f for substr in ['phase_adj'])]\n        features_to_scale = [f for f in fixed_order_features if f not in features_to_exclude]\n        print(features_to_scale)\n        all_features = np.concatenate( (meta_cols, fixed_order_features) )\n        \n        for f in train_df_clean.columns:\n            if f not in all_features:\n                print(f)\n\n        train_df_clean = train_df_clean[all_features]\n\n        scaler = StandardScaler().fit(train_df_clean[features_to_scale].to_numpy())\n        joblib.dump(scaler, os.path.join(Config.EXPORT_DIR, \"scaler.pkl\") )\n\n        train_sequences = train_df_clean.groupby('sequence_id')\n        print(train_df_clean.columns)\n\n        cols = {\n            #'train': train_cols,\n            'meta': meta_cols,\n            #'features': features_cols,\n            'imu': imu_cols,\n            'tof': tof_cols,\n            'thm': thm_cols\n        }\n        with open(os.path.join(Config.EXPORT_DIR, 'cols.pkl'), 'wb') as f:\n            pickle.dump(cols, f)\n\n\n        X, y = build_train_test_data(train_sequences, cols)\n        return X, y\n\n\n\ndef get_info(data_sequences, demograph, seq_id, print_data = False):\n    # Filter rows with the given sequence_id\n    #seq_id = 'SEQ_051475'\n    sequence_data = data_sequences.get_group(seq_id)\n\n    subject_id = sequence_data['subject'].iloc[0]\n    subject_demographics = demograph[demograph['subject'] == subject_id]\n\n    seq_info = sequence_data[\n        [\"sequence_id\", \"subject\", \"orientation\", \"gesture\", \"gesture_id\", \"sequence_type\"]\n    ].head(1).squeeze() \n    demo_info = subject_demographics[\n        [\"adult_child\", \"age\", \"sex\", \"handedness\", 'height_cm', 'shoulder_to_wrist_cm', 'elbow_to_wrist_cm']\n    ].head(1).squeeze()\n    demo_info[\"adult_child\"] = {0: \"child\", 1: \"adult\"}.get(demo_info[\"adult_child\"], \"unknown\")\n    demo_info[\"sex\"] = {0: \"female\", 1: \"male\"}.get(demo_info[\"sex\"], \"unknown\")\n    demo_info[\"handedness\"] = {0: \"left-handed\", 1: \"right-handed\"}.get(demo_info[\"handedness\"], \"unknown\")\n    combined = pd.concat([seq_info, demo_info])\n    if print_data:\n        display(combined.to_frame(name='Value'))\n    return combined\n\ndef get_info_v2(demograph, seq_id, seq_id_to_subject, print_data = False):\n    subject_id = seq_id_to_subject[seq_id]\n    subject_demographics = demograph[demograph['subject'] == subject_id]\n\n    demo_info = subject_demographics[\n        [\"adult_child\", \"age\", \"sex\", \"handedness\", 'height_cm', 'shoulder_to_wrist_cm', 'elbow_to_wrist_cm']\n    ].head(1).squeeze()\n    demo_info[\"adult_child\"] = {0: \"child\", 1: \"adult\"}.get(demo_info[\"adult_child\"], \"unknown\")\n    demo_info[\"sex\"] = {0: \"female\", 1: \"male\"}.get(demo_info[\"sex\"], \"unknown\")\n    demo_info[\"handedness\"] = {0: \"left-handed\", 1: \"right-handed\"}.get(demo_info[\"handedness\"], \"unknown\")\n    if print_data:\n        display(demo_info.to_frame(name='Value'))\n    return demo_info\n\n\ndef pad_and_truncate(X_batch, maxlen, padding_value=0.0, dtype=torch.float32):\n    padded_batch = []\n    for seq in X_batch:\n        seq = torch.tensor(seq, dtype=dtype)\n        length = seq.size(0)\n\n        # Truncate\n        if length > maxlen:\n            seq = seq[:maxlen]\n        # Pad\n        elif length < maxlen:\n            pad_len = maxlen - length\n            padding = torch.full((pad_len, *seq.shape[1:]), padding_value, dtype=dtype)\n            seq = torch.cat([seq, padding], dim=0)\n\n        padded_batch.append(seq)\n\n    return torch.stack(padded_batch)  # [batch_size, maxlen, features]\n\ndef build_train_test_data(data_sequences, cols, mask_gesture = False):\n    X_batch, y_batch, len_seq = [], [], []\n    features = np.concatenate( (cols['imu'], cols['thm'], cols['tof']) )\n    features_to_exclude = [f for f in features if any(substr in f for substr in ['phase_adj'])]\n    features_to_scale = [f for f in features if f not in features_to_exclude]\n\n    idx_to_scale = np.where(np.isin(features, features_to_scale))[0]\n    #idx_to_exclude = np.where(np.isin(features, features_to_exclude))[0]\n\n    seq_ids = []\n    for seq_id, data_sequence in data_sequences:\n        if mask_gesture:\n            gesture_phase = data_sequence['phase'] == 'Gesture'\n            sequence = data_sequence[features][gesture_phase]\n        else:\n            sequence = data_sequence[features]\n        \n        sequence = sequence.to_numpy()\n\n        # Fit and transform only those columns\n        scaler = joblib.load( os.path.join(Config.EXPORT_DIR, \"scaler.pkl\") )\n        if len(sequence) > 0:\n            sequence[:, idx_to_scale] =  scaler.transform(sequence[:, idx_to_scale])\n\n        #print(sequence[['linear_acc_ratio_svd_0', 'linear_acc_ratio_svd_1']])\n        #cols_to_scale = [c for c in cols['imu'] if c.startswith('acc_')]\n        #sequence[cols_to_scale] = scaler.fit_transform(sequence[cols_to_scale])\n\n        X_batch.append(sequence)\n        seq_ids.append(seq_id)\n        y_batch.append(data_sequence['gesture_id'].iloc[0])\n        len_seq.append(len(sequence))\n\n    ### labels one-hot categorical ###\n    y_final = torch.tensor(y_batch)\n    #y_final = F.one_hot(y_torch, num_classes = num_classes).float()\n    \n    ### pad and truncate sequences to the 95 percentile\n    pad_len_seq = int(np.percentile(len_seq, Config.PERCENTILE))\n    X_final = pad_and_truncate(X_batch, maxlen=pad_len_seq)\n\n    return X_final, y_final #, seq_ids\n\n\n### COMPETITION METRIC ###\n\ndef competition_metric(y_true, y_pred) -> tuple:\n    \"\"\"Calculate the competition metric (Binary F1 + Macro F1) / 2\"\"\"\n    BFRB_gesture = [0, 1, 3, 4, 6, 7, 9, 10]\n    #non_BFRB_gesture = [2, 5, 8, 11, 12, 13, 14, 15, 16, 17]\n     \n    # Binary F1: BFRB vs non-BFRB\n    binary_f1 = f1_score(\n        np.where(np.isin(y_true, BFRB_gesture), 1, 0),\n        np.where(np.isin(y_pred, BFRB_gesture), 1, 0),\n        zero_division=0.0,\n    )\n\n    binary_recall =  recall_score(\n        np.where(np.isin(y_true, BFRB_gesture), 1, 0),\n        np.where(np.isin(y_pred, BFRB_gesture), 1, 0),\n        zero_division=0.0,\n    )\n    \n    # Macro F1: specific gesture classification (only for BFRB gestures)\n    macro_f1 = f1_score(\n        np.where(np.isin(y_true, BFRB_gesture), y_true, 99),  # Map non-BFRB to 99\n        np.where(np.isin(y_pred, BFRB_gesture), y_pred, 99),  # Map non-BFRB to 99\n        average=\"macro\", \n        zero_division=0.0,\n    )\n    \n    # Final competition score\n    final_score = 0.5 * (binary_f1 + macro_f1)\n    \n    return final_score, binary_recall, macro_f1\n\ndef reset_seed(seed=42):\n    np.random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:26.757144Z","iopub.execute_input":"2025-08-01T13:55:26.757569Z","iopub.status.idle":"2025-08-01T13:55:26.845450Z","shell.execute_reply.started":"2025-08-01T13:55:26.757552Z","shell.execute_reply":"2025-08-01T13:55:26.844665Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom scipy.spatial.transform import Rotation as R\nimport glob\nfrom collections import Counter\n\n\nclass Conv1DAutoencoder(nn.Module):\n    def __init__(self, input_channels, hidden_dim = 16, latent_dim=32, drop = 0.3):\n        super().__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv1d(input_channels, 4 * hidden_dim, kernel_size=5, stride=4, padding=2),  # -> (B, 32, L/2)\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.Conv1d(4 * hidden_dim, 2 * hidden_dim, kernel_size=5, stride=4, padding=2),           # -> (B, 64, L/4)\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.Conv1d(2* hidden_dim, hidden_dim, kernel_size=5, stride=2, padding=2),          # -> (B, 128, L/8)\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=2, padding=2),              #nn.AdaptiveAvgPool1d(1),                                         # -> (B, 128, 1)\n        )\n        self.latent = nn.Linear(28 * hidden_dim, latent_dim)\n\n        # Decoder\n        self.decoder_fc = nn.Linear(latent_dim, 28 * hidden_dim) # (B, 128)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=5, stride=2, padding=2, output_padding=1),  #(B, 64, 11)\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.ConvTranspose1d(hidden_dim, 2 * hidden_dim, kernel_size=5, stride=2, padding=2, output_padding=1),   # (B, 64, 41)\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.ConvTranspose1d(2 * hidden_dim, 4 * hidden_dim, kernel_size=5, stride=4, padding=2),\n            nn.ReLU(),\n            nn.Dropout(p = drop),\n            nn.ConvTranspose1d(4 * hidden_dim, input_channels, kernel_size=5, stride=4, padding=2, output_padding=1),\n            #nn.Tanh()  # Assuming normalized input\n        )\n        self.hidden_dim = hidden_dim\n\n    def forward(self, x):\n        z = self.encoder(x)\n        z = z.reshape(z.shape[0], -1)\n        z = self.latent(z)\n        x_recon = self.decoder_fc(z)\n        x_recon = x_recon.unsqueeze(-1).reshape(-1, self.hidden_dim, 28)\n        #print(x_recon.shape)\n        x_recon = self.decoder(x_recon)\n        return x_recon\n\n\nclass LSTMWithAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=False)\n        self.attn = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Dropout(p = 0.3),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x: [B, T, F]\n        lstm_out, _ = self.lstm(x)  # lstm_out: [B, T, H]\n\n        attn_scores = self.attn(lstm_out).squeeze(-1)  # [B, T]\n        attn_weights = torch.softmax(attn_scores, dim=1)  # [B, T]\n\n        # Weighted sum\n        context = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # [B, H]\n\n        return context\n\n\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_dim, bias_strength = 5.):\n        super().__init__()\n        # self.attn = nn.Sequential(\n        #     nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1),\n        #     nn.Tanh(),\n        #     nn.Conv1d(hidden_dim, 1, kernel_size=1)\n        # )\n        # self.attn = nn.Sequential(\n        #     nn.Conv1d(hidden_dim, 1, kernel_size=1),  # [B, 1, T]\n        #     nn.Softmax(dim=-1)\n        # )\n        self.attn = nn.Sequential(\n            nn.Linear(hidden_dim, 1),  # [B, 1, T]\n            nn.Tanh()\n        )\n        self.bias_strength = bias_strength\n        self.weights = None\n\n    def forward(self, x, phase_adj = None):\n        # x: [B, hidden_dim, T]\n        scores = self.attn(x.permute(0, 2, 1)).squeeze(-1)  # [B, T]\n\n        if phase_adj is not None:\n            #bias = (phase_adj.float() * self.bias_strength)  # [B, T]\n            scores = scores #+ bias\n\n        weights = F.softmax(scores, dim=1)  # [B, T]\n        #weights = scores\n        self.weights = weights\n        pooled = torch.sum(x * weights.unsqueeze(1), dim=2)  # [B, hidden_dim]\n        return pooled\n\nclass IMUEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n            #nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True), #inplace=True\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p=0.3), \n            #nn.MaxPool1d(kernel_size=2, stride=2),  # halves time length\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            #nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(hidden_dim),\n            #nn.Dropout(p=0.2), \n            #nn.MaxPool1d(kernel_size=2, stride=2),   # halves again → total /4\n        )\n\n    def forward(self, x):\n        # x: [B, T, input_dim] → [B, input_dim, T]\n        x = x.permute(0, 2, 1)\n        out = self.net(x)  # [B, hidden_dim, T]\n        return out\n\nclass OptionalEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, norm = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p=0.3),\n            #nn.MaxPool1d(kernel_size=2, stride=2),\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(hidden_dim),\n            #nn.Dropout(p=0.2),\n            #nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n        self.norm = norm\n\n    def forward(self, x, mask):\n        # x: [B, T, input_dim] → [B, input_dim, T]\n        x = x.permute(0, 2, 1)\n        out = self.net(x)  # [B, hidden_dim, T/4]\n\n        # Adjust mask accordingly by downsampling (average pooling)\n        # mask: [B, T]\n        #mask = mask.unsqueeze(1).float()  # [B, 1, T]\n        #mask = F.avg_pool1d(mask, kernel_size=2, stride=2)  # [B, 1, T/2]\n        #mask = F.avg_pool1d(mask, kernel_size=2, stride=2)  # [B, 1, T/4]\n        #mask = mask.squeeze(1)  # [B, T/4]\n\n        #out = out * mask.unsqueeze(1)  # [B, hidden_dim, T]\n\n        #Normalize by sum of mask per timestep (avoid div zero)\n        if self.norm: \n            norm_mask = mask.sum(dim=1, keepdim=True).clamp(min=1e-6)  # [B, 1]\n            out = out / norm_mask.unsqueeze(1)  # broadcast on hidden_dim\n\n        return out\n\nclass TabularEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim)\n        )\n    def forward(self, x, seq_len):\n        # x: [B, n_feats]\n        emb = self.net(x)  # [B, hidden_dim]\n        # Expand along time dimension to [B, hidden_dim, seq_len]\n        emb = emb.unsqueeze(2).expand(-1, -1, seq_len)\n        return emb\n    \n\nclass TOFEncoder3DWithSpatialAttention(nn.Module):\n    def __init__(self, in_channels=5, out_channels=64, hidden_dim=128, H=8, W=8):\n        super().__init__()\n\n        # 3D CNN to process [B, 5, T, 8, 8]\n        self.conv3d = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv3d(out_channels, hidden_dim, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.BatchNorm3d(hidden_dim),\n            nn.ReLU(inplace=True),\n        )\n\n        # Spatial attention: per pixel over each 8x8 grid at each time step\n        self.spatial_attn = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim // 2, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim // 2, 1, kernel_size=1),  # attention logits per pixel\n        )\n\n        self.H = H\n        self.W = W\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, 5, T, 8, 8]\n        returns: [B, hidden_dim, T] (aggregated per timestep)\n        \"\"\"\n        B, C, T, H, W = x.shape\n\n        # Apply 3D conv\n        feat = self.conv3d(x)  # [B, hidden_dim, T, H, W]\n\n        # Reshape for spatial attention per time step\n        feat_reshaped = feat.permute(0, 2, 1, 3, 4).contiguous()  # [B, T, C, H, W]\n        feat_flat = feat_reshaped.view(B * T, -1, H, W)  # [B*T, C, H, W]\n\n        # Spatial attention scores\n        attn_logits = self.spatial_attn(feat_flat)  # [B*T, 1, H, W]\n        attn_scores = F.softmax(attn_logits.view(B * T, -1), dim=-1).view(B * T, 1, H, W)  # [B*T, 1, H, W]\n\n        # Apply attention\n        weighted_feat = feat_flat * attn_scores  # [B*T, C, H, W]\n        aggregated = weighted_feat.view(B, T, -1, H * W).sum(dim=-1)  # [B, T, C]\n\n        # Transpose to [B, C, T] to match other branches\n        aggregated = aggregated.permute(0, 2, 1)\n\n        return aggregated  # [B, hidden_dim, T]\n\nclass TOFEncoder(nn.Module):\n    def __init__(self, hidden_dim, C, H, W, C_TOF_RAW = False, norm = False):\n        super().__init__()\n        if C_TOF_RAW:\n            self.tof_spatial_weight = nn.Parameter(torch.ones(1, 5, H, W))  # Learnable\n        else:\n            self.tof_spatial_weight = nn.Parameter(torch.ones(1, 1, H, W))  # Learnable\n\n        self.spatial_pool = nn.AdaptiveAvgPool2d( 1 )  # or MaxPool2d or Flatten\n        self.tof_post = nn.Sequential(\n            nn.Linear(C , hidden_dim),  # or Conv1D\n            nn.ReLU(),\n        )\n        self.H = H\n        self.W = W\n        self.C = C\n\n        self.norm = norm\n    \n    def forward(self, tof_raw, mask_ones):\n        B, T, _ = tof_raw.shape\n\n        tof_raw =  tof_raw.reshape(B, T, self.C, self.H, self.W) #[B, T, 5, 8, 8] \n        #tof_raw = tof_raw.permute(0, 2, 1, 3, 4)                \n\n        tof_raw_weighted = (tof_raw * self.tof_spatial_weight)#.view(-1, self.C, self.H, self.W)  # Broadcasting over batch and channel\n        #print(tof_raw_weighted.shape)\n        pooled = self.spatial_pool(tof_raw_weighted)#.view(B, T, -1)  # [B, T, 5 * 2 * 2]\n        #print(pooled.shape)\n        pooled = pooled.squeeze(-1).squeeze(-1)#.permute(0, 2, 1)  \n        tof_raw_feat = self.tof_post(pooled)  # [B, T, hidden_dim]  \n\n        if self.norm:\n            norm_mask = mask_ones.sum(dim=1, keepdim=True).clamp(min=1e-6)  # [B, 1]\n            tof_raw_feat = tof_raw_feat / norm_mask.unsqueeze(1)  # broadcast on hidden_dim\n\n        return tof_raw_feat.permute(0, 2, 1) # [B, hidden_dim, T]  \n\n\nclass TOFEncoderTemporalBeforePool(nn.Module):\n    def __init__(self, hidden_dim, C, H, W):\n        super().__init__()\n\n        self.C = C\n        self.H = H\n        self.W = W\n\n        self.temporal_attn = nn.Sequential(\n            nn.Conv2d(C, 1, kernel_size=1),  # [B*T, 1, H, W]\n            nn.Sigmoid()\n        )\n\n        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.tof_post = nn.Sequential(\n            nn.Linear(C, hidden_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, tof_raw):\n        B, T, _ = tof_raw.shape\n        tof_raw = tof_raw.view(B, T, self.C, self.H, self.W)  # [B, T, C, H, W]\n        tof_raw_2d = tof_raw.view(B*T, self.C, self.H, self.W)  # Merge batch & time\n\n        # Compute per-frame spatial attention weights\n        attn_maps = self.temporal_attn(tof_raw_2d)  # [B*T, 1, H, W]\n        tof_weighted = tof_raw_2d * attn_maps  # Apply attention\n        tof_weighted = tof_weighted.view(B, T, self.C, self.H, self.W)\n\n        # Pool and project\n        pooled = self.spatial_pool(tof_weighted).squeeze(-1).squeeze(-1)  # [B, T, C]\n        tof_feat = self.tof_post(pooled)  # [B, T, hidden_dim]\n\n        return tof_feat.permute(0, 2, 1)  # [B, hidden_dim, T]\n\n\nclass GatedFusion(nn.Module):\n    def __init__(self, hidden_dim, num_modalities):\n        super().__init__()\n        self.gate = nn.Linear(hidden_dim * num_modalities, num_modalities)\n\n    def forward(self, features_list):\n        # features_list: list of [B, hidden_dim, T]\n        concat = torch.cat(features_list, dim=1)  # [B, hidden_dim * M, T]\n        concat_t = concat.permute(0, 2, 1)        # [B, T, hidden_dim * M]\n        gate_weights = torch.sigmoid(self.gate(concat_t))  # [B, T, M]\n\n        gated_feats = []\n        for i, f in enumerate(features_list):\n            f_t = f.permute(0, 2, 1)  # [B, T, hidden_dim]\n            w = gate_weights[:, :, i].unsqueeze(-1)  # [B, T, 1]\n            gated_feats.append(f_t * w)\n        fused = sum(gated_feats)  # [B, T, hidden_dim]\n        return fused.permute(0, 2, 1)  # [B, hidden_dim, T]\n\nclass AttentionFusion(nn.Module):\n    def __init__(self, hidden_dim, num_modalities=3):\n        super().__init__()\n        self.query = nn.Linear(hidden_dim, hidden_dim)\n        self.keys = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_modalities)])\n        self.values = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_modalities)])\n        self.scale = hidden_dim ** 0.5\n\n    def forward(self, inputs):\n        \"\"\"\n        inputs: list of [B, hidden_dim, T] tensors\n        output: [B, hidden_dim, T] fused tensor\n        \"\"\"\n        # Compute shared query\n        #stacked_inputs = torch.stack(inputs, dim=1)  # [B, M, hidden_dim, T]\n        #B, M, C, T = stacked_inputs.shape\n\n        query = self.query(inputs[0].permute(0, 2, 1))  # [B, T, hidden_dim]\n        keys = [key(x.permute(0, 2, 1)) for key, x in zip(self.keys, inputs)]   # each: [B, T, hidden_dim]\n        values = [val(x.permute(0, 2, 1)) for val, x in zip(self.values, inputs)]  # each: [B, T, hidden_dim]\n\n        key_tensor = torch.stack(keys, dim=1)     # [B, M, T, hidden_dim]\n        value_tensor = torch.stack(values, dim=1) # [B, M, T, hidden_dim]\n\n        # Attention: dot product over modalities\n        attn_scores = torch.einsum('btc,bmtc->btm', query, key_tensor) / self.scale  # [B, T, M]\n        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, T, M]\n\n        # Weighted sum over modalities\n        fused = torch.einsum('btm,bmtc->btc', attn_weights, value_tensor)  # [B, T, hidden_dim]\n        return fused.permute(0, 2, 1)  # back to [B, hidden_dim, T]\n\n    \nclass GlobalGestureClassifier(nn.Module):\n    def __init__(self, \n                 imu_dim, \n                 hidden_dim, \n                 num_classes, \n                 thm_tof_dim = None, \n                 tof_raw_dim = None, \n                 C_TOF_RAW = False,\n                 norm_TOF_THM = True,\n                 norm_TOF_RAW = False,\n                 attention_for_fusion = True,\n                 attention_pooled = True,\n                 ): # tabular_dim = None\n        super().__init__()\n\n        if thm_tof_dim is None:\n            thm_tof_dim = imu_dim\n        \n        self.thm_tof_dim = thm_tof_dim\n        self.tof_raw_dim = tof_raw_dim\n\n        self.attention_pooled = attention_pooled\n        self.attention_for_fusion = attention_for_fusion\n\n        self.imu_encoder = IMUEncoder(imu_dim, hidden_dim)\n        if self.attention_pooled:\n            self.attn_pool = AttentionPooling(hidden_dim)\n\n        self.thm_tof_encoder = OptionalEncoder(thm_tof_dim, hidden_dim, norm=norm_TOF_THM)\n        self.tof_pixels = TOFEncoder(hidden_dim, C = 4, H = 8, W = 8, C_TOF_RAW=C_TOF_RAW, norm=norm_TOF_RAW)\n        #self.tof_pixels = TOFEncoderTemporalBeforePool(hidden_dim, C = 5, H = 8, W = 8)\n#         self.tof_spatial_weight = nn.Parameter(torch.ones(1, 1, 8, 8))  # Learnable\n#         self.spatial_pool = nn.AdaptiveAvgPool2d(1)  # or MaxPool2d or Flatten\n#         self.tof_post = nn.Sequential(\n#             nn.Linear(5, hidden_dim),  # or Conv1D\n#             nn.ReLU(),\n# )\n\n        self.gated_fusion = GatedFusion(hidden_dim, num_modalities=3)\n        if self.attention_for_fusion:\n            self.attention_fusion = AttentionFusion(hidden_dim, num_modalities=3)\n            self.alpha = nn.Parameter(torch.tensor(0.0))  # sigmoid(0) = 0.5\n        \n        self.final_fusion = nn.Sequential(\n            nn.Linear(2 * hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3)\n        )\n        #self.bilstm = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n        #self.classifier_rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n\n        self.classifier_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.3),\n            nn.Linear(hidden_dim, num_classes),\n            #nn.Softmax()\n        )\n\n        self.imu_head = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Dropout(p = 0.3),\n            nn.Linear(hidden_dim, num_classes)\n        )\n\n    def forward(self, imu, thm_tof=None, tof_raw = None): #, tabular_feats=None\n        B, T, _ = imu.shape\n\n        imu_feat = self.imu_encoder(imu)  # [B, hidden_dim, T]\n        pooled_imu = imu_feat.mean(dim= 2)\n        logits_imu = self.imu_head(imu_feat)\n\n        if thm_tof is None:\n            thm_tof = torch.zeros_like(imu)\n            tof_mask = torch.zeros(B, T, device=imu.device)\n        else:\n            tof_mask = torch.ones( B, T, device = thm_tof.device ) # (~torch.isnan(thm_tof).any(dim=2)).float() #(thm_tof != 0).float() #\n\n        if tof_raw is None:\n            tof_raw = torch.zeros_like(imu)\n            tof_raw_mask = torch.zeros(B, T, device=imu.device)\n        else:\n            tof_raw_mask = torch.ones( B, T, device = thm_tof.device )\n\n\n        thm_tof_feat = self.thm_tof_encoder(thm_tof, tof_mask)  # [B, hidden_dim, T]\n        tof_raw_feat = self.tof_pixels(tof_raw, tof_raw_mask) # [B, hidden_dim, T]\n\n        gated =  self.gated_fusion([imu_feat, thm_tof_feat, tof_raw_feat])  # [B, hidden_dim, T]\n\n        if self.attention_for_fusion:\n            attn =  self.attention_fusion([imu_feat, thm_tof_feat, tof_raw_feat])  # [B, hidden_dim, T]\n            alpha = torch.sigmoid(self.alpha).view(1, -1, 1)  # shape [1, C, 1] \n            fused = alpha * gated + (1 - alpha) * attn  # broadcast over B, T\n        else:\n            fused = gated\n\n        if self.attention_pooled:     \n            pooled_fused= self.attn_pool(fused) #fused.mean(dim= 2) #\n        else:\n            pooled_fused= fused.mean(dim= 2) #fused.mean(dim= 2) #\n\n\n        pooled = self.final_fusion(torch.cat([pooled_imu, pooled_fused], dim=1))\n        #pooled = torch.cat([pooled_imu, pooled_fused], dim=1)\n        out = self.classifier_head(pooled)  # [B, num_classes]\n\n        return out, logits_imu   \n\nclass MiniGestureClassifier(nn.Module):\n    def __init__(self, imu_dim, hidden_dim, num_classes):\n        super().__init__()\n\n        self.imu_encoder = IMUEncoder(imu_dim, hidden_dim)\n        self.attn_pool = AttentionPooling(hidden_dim)\n    \n\n        self.classifier_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p = 0.3),\n            nn.Linear(hidden_dim, num_classes),\n            #nn.Softmax()\n        )\n\n    def forward(self, imu, return_attention=False): #, phase_adj = None,\n        B, T, _ = imu.shape\n\n        imu_feat = self.imu_encoder(imu)  # [B, hidden_dim, T]\n\n        pooled = imu_feat.mean(dim=2)#   # [B, hidden_dim]\n\n        #pooled = self.attn_pool(imu_feat)\n\n        out = self.classifier_head(pooled)  # [B, num_classes]\n\n        return (out, self.attn_pool.weights) if return_attention else out\n    \nclass MiniGestureLSTMClassifier(nn.Module):\n    def __init__(self, imu_dim, imu_dim_lstm, hidden_dim, lstm_hidden_dim, num_classes):\n        super().__init__()\n\n        self.imu_encoder = IMUEncoder(imu_dim, hidden_dim)\n        self.lstm_attn = LSTMWithAttention(imu_dim_lstm, lstm_hidden_dim)\n        \n        fused_dim = hidden_dim + lstm_hidden_dim\n\n        self.classifier_head = nn.Sequential(\n            nn.Linear(fused_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p = 0.3),\n            nn.Linear(hidden_dim, num_classes),\n            #nn.Softmax()\n        )\n\n    def forward(self, imu): #, phase_adj = None,\n        #B, T, _ = imu.shape\n\n        imu_cnn_out = self.imu_encoder(imu)  # [B, hidden_dim, T]\n        imu_pooled = imu_cnn_out.mean(dim=2) # [B, hidden_dim]\n        imu_lstm_out = self.lstm_attn(imu)  # [B, H]\n\n        fused = torch.cat([imu_pooled, imu_lstm_out], dim=1)  # [B, hidden_dim + H]\n        out = self.classifier_head(fused)\n\n        return out\n    \n\nclass EarlyStopping:\n    def __init__(self, patience=5, mode='max', restore_best_weights=True, verbose=False, logger = None):\n        self.patience = patience\n        self.mode = mode\n        self.restore_best_weights = restore_best_weights\n        self.verbose = verbose\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        self.best_model_state = None\n        self.logger = logger\n\n    def __call__(self, current_score, model):\n        if self.mode == 'max':\n            score_improved = self.best_score is None or current_score > self.best_score\n        else:  # 'min'\n            score_improved = self.best_score is None or current_score < self.best_score\n\n        if score_improved:\n            self.best_score = current_score\n            self.counter = 0\n            if self.restore_best_weights:\n                self.best_model_state = model.state_dict()\n            if self.verbose:\n                if self.logger is not None:\n                    self.logger.info(f\"EarlyStopping: Improvement found, saving model with score {current_score:.4f}\")\n                else:\n                    print(f\"EarlyStopping: Improvement found, saving model with score {current_score:.4f}\")\n        else:\n            self.counter += 1\n            if self.verbose:\n                if self.logger is not None:\n                    self.logger.info(f\"EarlyStopping: No improvement for {self.counter} epoch(s)\")\n                else:\n                    print(f\"EarlyStopping: No improvement for {self.counter} epoch(s)\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    if self.logger is not None:\n                        self.logger.info(\"EarlyStopping: Stopping early.\")\n                    else:\n                        print(\"EarlyStopping: Stopping early.\")\n                if self.restore_best_weights and self.best_model_state is not None:\n                    model.load_state_dict(self.best_model_state)\n\n\n\nclass SensorDataset(Dataset):\n    def __init__(self, X, y, imu_dim, alpha = 0., augment = None, training = True):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.alpha = alpha\n        self.augment = augment\n        self.training = training\n        self.imu_dim = imu_dim\n\n    def __len__(self):\n        return len(self.X) \n\n    def __getitem__(self, idx):\n        x1, y1 = self.X[idx], self.y[idx]\n        y1_onehot = torch.nn.functional.one_hot(y1, num_classes=18).float()\n\n        if self.training and self.augment:\n            x1 = x1.numpy().copy()\n            x1 = self.augment(x1, imu_dim = self.imu_dim)\n            x1 = torch.tensor(x1,  dtype=torch.float32)\n            \n        if self.alpha > 1e-6:\n            rand_idx = np.random.randint(0, len(self.X) - 1)\n            x2, y2 = self.X[rand_idx], self.y[rand_idx]\n\n            if self.training and self.augment:\n                x2 = x2.numpy().copy()\n                x2 = self.augment(x2, imu_dim=self.imu_dim)\n                x2 = torch.tensor(x2, dtype=torch.float32)\n\n            y2_onehot = torch.nn.functional.one_hot(y2, num_classes=18).float()\n\n            # Generate lambda from Beta distribution and ensure alpha > 0.\n            lam = np.random.beta(self.alpha, self.alpha)\n            lam = max(0, min(1, lam))\n\n            x1 = lam * x1 + (1 - lam) * x2\n            y1_onehot = lam * y1_onehot + (1 - lam) * y2_onehot\n        \n        return x1, y1_onehot\n\nclass TrackingSampler(torch.utils.data.Sampler):\n    def __init__(self, base_sampler):\n        self.base_sampler = base_sampler\n        self.sampled_indices = []\n\n    def __iter__(self):\n        self.sampled_indices = list(self.base_sampler)  # Store for external access\n        return iter(self.sampled_indices)\n\n    def __len__(self):\n        return len(self.base_sampler)\n\nclass DeviceRotationAugment:\n    def __init__(self,\n                X, y, seqs,       \n                seqs_by_subject,\n                selected_features,\n                x_rot_range = (0, 30), # (0, 45)\n                y_rot_range = (0, 30), # (0, 45)\n                p_rotation = 0.4,\n                small_rotation = 2\n                ):     \n        \n        self.features_to_rotate = [\n        ['acc_x', 'acc_y', 'acc_z'],\n        ['acc_x_world', 'acc_y_world', 'acc_z_world'],\n        ['linear_acc_x', 'linear_acc_y', 'linear_acc_z'],\n        ['rotvec_x', 'rotvec_y', 'rotvec_z'],\n        ['ang_vel_x', 'ang_vel_y', 'ang_vel_z'],\n        ['X_world_x', 'X_world_y', 'X_world_z'], \n        ['Y_world_x', 'Y_world_y', 'Y_world_z'],\n        ['Z_world_x', 'Z_world_y', 'Z_world_z'],\n        ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n        ]\n        \n        self.seqs_by_subject = seqs_by_subject \n        self.p_rotation = p_rotation\n        self.selected_features = selected_features\n        self.x_rot_range = x_rot_range\n        self.y_rot_range = y_rot_range\n        self.small_rotation = small_rotation\n\n        self.X =  torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.seqs = seqs\n        self.count = 0\n        self.iter = 2\n\n    def random_angles_by_seq(self):\n        unique_subjects = list(self.seqs_by_subject.keys())\n        # Assign a consistent random Y angle per subject\n        subject_to_angle = {\n            subj:  (np.random.uniform(*self.x_rot_range), np.random.uniform(*self.y_rot_range)) #np.random.choice(y_range)\n            for subj in unique_subjects\n        }\n\n        random_small_angles_by_subject = {\n            subj: np.random.uniform(-self.small_rotation, self.small_rotation, size=len(seqs))\n            for subj, seqs in self.seqs_by_subject.items()\n        }\n\n\n        subject_for_seq = {\n        seq_id: (i, subj) for subj, seq_ids in self.seqs_by_subject.items() for i, seq_id in enumerate(seq_ids)\n        }\n\n        seq_to_angle = {\n            seq_id: (subject_to_angle[subj][0] + random_small_angles_by_subject[subj][i], subject_to_angle[subj][1] + random_small_angles_by_subject[subj][i])\n            for seq_id, (i, subj) in subject_for_seq.items()\n        }\n\n        return seq_to_angle\n\n    def apply_rotation(self, \n                       x: torch.tensor, \n                       ax: str, \n                       seq_id: str,\n                       seqs_to_angle) -> np.ndarray:\n        x_copy = x.numpy().copy()\n        rot_x, rot_y = seqs_to_angle.get(seq_id, (0.0, 0.0))\n        if ax == 'x':\n            rot = R.from_euler(ax, rot_x, degrees=True)\n        if ax == 'y':\n            rot = R.from_euler(ax, rot_y, degrees=True)\n        if ax == 'z':\n            rot = R.from_euler(ax, 180, degrees=True)\n        if ax == 'zx':\n            rot = R.from_euler('z', 180, degrees=True) *  R.from_euler('x', rot_x, degrees=True)\n        if ax == 'zy':\n            rot = R.from_euler('z', 180, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n        if ax == 'xy':\n            rot = R.from_euler('x', rot_x, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n        if ax == 'zxy':\n            rot = R.from_euler('z', 180, degrees=True) * R.from_euler('x', rot_x, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n\n        for feats in self.features_to_rotate:\n            idx_rotate = np.where(np.isin(self.selected_features, feats))[0]\n            if len(idx_rotate) == 0:\n                continue\n\n            if not any('rot_' in f for f in feats):\n                rotated = rot.apply(x_copy[:, idx_rotate])\n                x_copy[:, idx_rotate] = rotated\n            else:\n                init_quat = x_copy[:, idx_rotate]\n                mask = np.linalg.norm(init_quat, axis=1) < 1e-6\n                R_orig = R.from_quat(init_quat[~mask])\n                R_new = rot * R_orig\n                new_quat = np.zeros_like(init_quat)\n                new_quat[~mask] = R_new.as_quat()\n                x_copy[:, idx_rotate] = new_quat\n\n        return x_copy\n    \n    def apply_specific_rotation( \n                       x, \n                       ax: str, \n                       rots,\n                       selected_features,\n                       features_to_rotate) -> np.ndarray:\n        x_copy = x.numpy().copy()\n        rot_x, rot_y, rot_z = rots\n        if ax == 'x':\n            rot = R.from_euler(ax, rot_x, degrees=True)\n        if ax == 'y':\n            rot = R.from_euler(ax, rot_y, degrees=True)\n        if ax == 'z':\n            rot = R.from_euler(ax, rot_z, degrees=True)\n        if ax == 'zx':\n            rot = R.from_euler('z', rot_z, degrees=True) *  R.from_euler('x', rot_x, degrees=True)\n        if ax == 'xz':\n            rot = R.from_euler('x', rot_x, degrees=True) *  R.from_euler('z', rot_z, degrees=True)\n        if ax == 'zy':\n            rot = R.from_euler('z', rot_z, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n        if ax == 'xy':\n            rot = R.from_euler('x', rot_x, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n        if ax == 'zxy':\n            rot = R.from_euler('z', rot_z, degrees=True) * R.from_euler('x', rot_x, degrees=True) *  R.from_euler('y', rot_y, degrees=True)\n\n        for feats in features_to_rotate:\n            idx_rotate = np.where(np.isin(selected_features, feats))[0]\n            if len(idx_rotate) == 0:\n                continue\n\n            if not any('rot_' in f for f in feats):\n                rotated = rot.apply(x_copy[:, idx_rotate])\n                x_copy[:, idx_rotate] = rotated\n            else:\n                init_quat = x_copy[:, idx_rotate]\n                mask = np.linalg.norm(init_quat, axis=1) < 1e-6\n                R_orig = R.from_quat(init_quat[~mask])\n                R_new = rot * R_orig\n                new_quat = np.zeros_like(init_quat)\n                new_quat[~mask] = R_new.as_quat()\n                x_copy[:, idx_rotate] = new_quat\n\n        return x_copy\n    \n    # ---------- master call ----------\n    def __call__(self,\n                 axes: list) -> np.ndarray:\n    \n\n        seqs_to_angle = self.random_angles_by_seq()\n        \n        augmented_X_tr = []\n        augmented_y_tr = []\n\n        for xx, yy, seq_id in zip(self.X, self.y, self.seqs):\n            augmented_X_tr.append(xx)\n            augmented_y_tr.append(yy)\n\n            # Reverse time (assuming time is dimension 0)\n            x_rotated = []\n            axes_choice = np.array(axes)\n            for ax in axes_choice: #self.iter\n                #if (np.random.random() < self.p_rotation) and (len(axes_choice) > 0):\n                #ax = axes_choice[i]  #np.random.choice(axes_choice) # ##\n                x_rotated.append(self.apply_rotation(xx, ax, seq_id, seqs_to_angle)) # subject_id, subject_to_angle)\n                    #axes_choice = np.delete(axes_choice, np.where(axes_choice == ax)) \n            if len(x_rotated) > 0:\n                self.count += len(x_rotated)\n                x_rotated = [torch.tensor(x) for x in x_rotated]\n                augmented_X_tr.extend(x_rotated)\n                augmented_y_tr.extend([yy] * len(x_rotated))\n\n\n        #augmented_X_tr = torch.tensor(augmented_X_tr)\n        augmented_X_tr = torch.stack(augmented_X_tr)\n        augmented_y_tr = torch.tensor(augmented_y_tr)  # Or use torch.stack if already tensors\n\n        # X_aug = my_aug.augment(self.X.numpy())  # shape preserved\n        # y_aug = self.y.clone()          # labels remain the same\n        \n        # X_aug = torch.cat([self.X, torch.from_numpy(X_aug)], dim=0)\n        # y_aug = torch.cat([self.y, y_aug], dim=0)\n        return augmented_X_tr, augmented_y_tr, self.count\n\n\nclass Augment:\n    def __init__(self,\n                 p_jitter=0.8, sigma=0.02, scale_range=[0.9,1.1],\n                 p_dropout=0.3,\n                 p_moda=0.5,\n                 drift_std=0.005,     \n                 drift_max=0.25):      \n        self.p_jitter  = p_jitter\n        self.sigma     = sigma\n        self.scale_min, self.scale_max = scale_range\n        self.p_dropout = p_dropout\n        self.p_moda    = p_moda\n\n        self.drift_std = drift_std\n        self.drift_max = drift_max\n\n\n    # ---------- Jitter & Scaling ----------\n    def jitter_scale(self, x: np.ndarray) -> np.ndarray:\n        noise  = np.random.randn(*x.shape) * self.sigma\n        scale  = np.random.uniform(self.scale_min,\n                                   self.scale_max,\n                                   size=(1, x.shape[1]))\n        return (x + noise) * scale\n\n    # ---------- Sensor Drop-out ----------\n    def sensor_dropout(self,\n                       x: np.ndarray,\n                       imu_dim: int) -> np.ndarray:\n\n        if np.random.random() < self.p_dropout:\n            x[:, imu_dim:] = 0.0\n        return x\n\n    def motion_drift(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n\n        T = x.shape[0]\n\n        drift = np.cumsum(\n            np.random.normal(scale=self.drift_std, size=(T, 1)),\n            axis=0\n        )\n        drift = np.clip(drift, -self.drift_max, self.drift_max)   \n\n        x[:, :6] += drift\n\n        if imu_dim > 6:\n            x[:, 6:imu_dim] += drift     \n        return x\n    \n\n    \n    # ---------- master call ----------\n    def __call__(self,\n                 x: np.ndarray,\n                 imu_dim: int) -> np.ndarray:\n        \n        if np.random.random() < self.p_jitter:\n            x = self.jitter_scale(x)\n\n        if np.random.random() < self.p_moda:\n            x = self.motion_drift(x, imu_dim)\n\n        x = self.sensor_dropout(x, imu_dim)\n        return x\n    \nclass EnsemblePredictor:\n    def __init__(self,  processing_dir, models_dir, device, params):\n        self.device = device\n        self.models = {\n            'hybrid_models': [],\n            'imu_only_models': [],\n            'imu_tof_thm_models': []\n            }\n        self.params = params\n        self.scaler = None\n        self.features = None\n        self.label_encoder = None\n        self.map_classes = None\n        self.inverse_map_classes = None\n        self.cols = None\n        self._load_models(models_dir, seed_CV_fold = params[\"SEED_CV_FOLD\"])\n        self._load_processing(processing_dir)\n\n    def _load_models(self, models_dir, seed_CV_fold = None):\n        model_files = {}\n        if seed_CV_fold is None:\n            model_files['hybrid_models'] = sorted(glob.glob(f\"{models_dir}/best_model_fold_*.pth\"))\n            model_files['imu_only_models'] = sorted(glob.glob(f\"{models_dir}/best_model_imu_only_fold_*.pth\"))\n            model_files['imu_tof_thm_models'] = sorted(glob.glob(f\"{models_dir}/best_model_imu_tof_thm_fold_*.pth\"))\n        else:\n            model_files['hybrid_models'] = sorted(glob.glob(f\"{models_dir}/best_model_fold_*_{seed_CV_fold}.pth\"))\n            model_files['imu_only_models'] = sorted(glob.glob(f\"{models_dir}/best_model_imu_only_fold_*_{seed_CV_fold}.pth\"))\n            model_files['imu_tof_thm_models'] = sorted(glob.glob(f\"{models_dir}/best_model_imu_tof_thm_fold_*_{seed_CV_fold}.pth\"))\n            \n        for key, models in model_files.items():\n            print(f\"{len(models)} {' '.join(key.split('_'))} have been found\")\n        \n        for key, models in model_files.items():\n            for model_file in models:\n                checkpoint = torch.load(model_file, map_location=self.device, weights_only=True)\n                \n                #model = MiniGestureClassifier(imu_dim=14, hidden_dim=128, num_classes=18)\n                model = GlobalGestureClassifier(imu_dim=self.params['imu_dim'], \n                                                thm_tof_dim=self.params['thm_tof_dim'], \n                                                tof_raw_dim=self.params['tof_raw_dim'],  \n                                                hidden_dim=self.params['HIDDEN_DIM'], \n                                                num_classes=self.params['N_CLASSES'], \n                                                C_TOF_RAW=self.params['C_TOF_RAW'],\n                                                norm_TOF_RAW=self.params['normalisation_TOF_RAW'],\n                                                norm_TOF_THM=self.params['normalisation_TOF_THM'],\n                                                attention_for_fusion=self.params['attention_for_fusion'],\n                                                attention_pooled= self.params['attention_pooled']\n                                            ) # MODEL            \n                model.load_state_dict(checkpoint) #['model_state_dict']\n                model.to(self.device)\n                model.eval()\n                self.models[key].append(model)\n\n    def _load_processing(self, processing_dir):\n        self.scaler = joblib.load(os.path.join(processing_dir, \"scaler.pkl\"))\n        self.label_encoder = joblib.load(os.path.join(processing_dir, \"label_encoder.pkl\"))\n        self.map_classes = {idx: cl for idx, cl in enumerate(self.label_encoder.classes_)}\n        self.inverse_map_classes = {cl: idx for idx, cl in enumerate(self.label_encoder.classes_)}\n\n        \n        file_path_cols = os.path.join(processing_dir, \"cols.pkl\")\n        with open(file_path_cols, 'rb') as f:\n            self.cols = pickle.load(f)\n        self.features = np.concatenate( (self.cols['imu'], self.cols['thm'], self.cols['tof']) ) \n\n        print(\"-> scaler, features, labels classes loaded\")\n        #print(f\"features = {self.features}\")\n    \n    def features_eng(self, df_seq: pd.DataFrame, demographics: pd.DataFrame):\n        df_seq = regularize_quaternions_per_sequence(df_seq)\n\n        ### -- ADD NEW FEATURES (IMU + AVERAGED TOF COLUMNS) --- \n        df_seq = df_seq.reset_index(drop=True)\n        df_seq = add_gesture_phase(df_seq)\n        df_seq = compute_acceleration_features(df_seq, demographics)\n        df_seq = compute_angular_features(df_seq, demographics)\n        df_seq = manage_tof(df_seq, demographics)\n        return df_seq\n\n    def scale_pad_and_transform_to_torch_sequence(self, df_seq, pad_length, is_imu_only = False):\n        ### -- Columns re-ordering to match train order\n        df_seq_features = df_seq[self.features].copy()\n\n        #has_nan_tof_thm = df_seq_features[ np.concatenate( (self.cols['tof'], self.cols['thm']) ) ].isnull().all(axis=1).all()\n        # if has_nan_tof_thm:\n        #     print(\"NaN values have been found in TOF and/or THM data\")\n        \n        has_nan_imu = df_seq_features[self.cols['imu']].isnull().any().any()\n        if has_nan_imu:\n            print(\"x IMU cols have NaN values. Shouldn't be the case! Check data!\")\n\n        ### -- Scale features and check NaN for IMU COLS  \n        np_seq_features  =  df_seq_features.to_numpy()\n        features_to_exclude = [f for f in self.features if any(substr in f for substr in ['phase_adj'])]\n        features_to_scale = [f for f in self.features if f not in features_to_exclude]\n        idx_to_scale = np.where(np.isin(self.features, features_to_scale))[0]\n        if len(np_seq_features) > 0:\n            np_seq_features[:, idx_to_scale] =  self.scaler.transform(np_seq_features[:, idx_to_scale])\n        \n        imu_features = [\n            'acc_x','acc_y','acc_z', 'rotvec_x', 'rotvec_y', 'rotvec_z', \n            'linear_acc_x', 'linear_acc_y', 'linear_acc_z', \n            'ang_vel_x', 'ang_vel_y', 'ang_vel_z', 'ang_dist',\n            'phase_adj'\n            ] \n\n        if is_imu_only:\n            idx_imu = [np.where(self.features == f)[0][0] for f in imu_features]    ### select features from selected_features above\n            np_seq_features = np_seq_features[:, idx_imu]\n        else:\n            selected_tof = [f for f in self.cols['tof'] if ('v' not in f) and ('tof_5' not in f)]\n            raw_tof = [f for f in self.cols['tof'] if ('v' in f) and ('tof_5' not in f)]\n\n            raw_tof_sorted = np.array([f'tof_{i}_v{j}' for i in range(1, 5) for j in range(64)])\n            check_all_pixels = np.array([f in raw_tof for f in raw_tof_sorted]   )            ### THM Features for later\n\n            if not np.all(check_all_pixels):\n                print(f\"missing pixel raw data in TOF data: {np.array(raw_tof_sorted)[~check_all_pixels]}\")\n            \n            raw_tof_sorted = list(raw_tof_sorted[check_all_pixels])\n            idx_imu = [np.where(self.features == f)[0][0] for f in imu_features]    ### select features from selected_features above\n            idx_tof = [np.where(self.features == f)[0][0] for f in selected_tof]                   ### TOF Features for later\n            idx_raw_tof = [np.where(self.features == f)[0][0] for f in raw_tof_sorted]                   ### TOF Features for later\n            idx_thm = [np.where(self.features == f)[0][0] for f in self.cols['thm'] if 'thm_5' not in f]               ### THM Features for later           ### THM Features for later\n            \n            idx_all = idx_imu + idx_thm + idx_tof + idx_raw_tof\n            np_seq_features = np_seq_features[:, idx_all]\n\n        seq = torch.tensor(np_seq_features, dtype=torch.float32)\n        length = seq.size(0)\n        # Truncate\n        if length >= pad_length:\n            seq = seq[:pad_length].unsqueeze(0)\n        # Pad\n        elif length < pad_length:\n            pad_len = pad_length - length\n            padding = torch.full((pad_len, *seq.shape[1:]), 0.0, dtype=torch.float32)\n            seq = torch.cat([seq, padding], dim=0).unsqueeze(0)\n\n        #print(f\"sequence has been scaled and padded. shape (1, T, F): {seq.shape}\")\n        return seq.to(self.device)\n\n    def predict(self, torch_seq, by_fold = None, models_to_use = ['hybrid_models', 'imu_only_models', 'imu_tof_thm_models']):\n    # torch_seq: [N, ...]  (N = batch size)\n\n        weights_models = {\n            'hybrid_models': 1.,\n            'imu_only_models': 0.75,\n            'imu_tof_thm_models': 0.5\n            }\n        \n        weights = {name: weights_models[name] for name in models_to_use}\n        weights = {name: w/sum(weights.values()) for name, w in weights.items()}\n\n        indices_branches = {\n            'imu': np.arange(self.params['imu_dim']), \n            'thm_tof': np.arange(self.params['imu_dim'],self.params['imu_dim'] + self.params['thm_tof_dim']), \n            'tof_raw': np.arange(self.params['imu_dim'] + self.params['thm_tof_dim'], self.params['imu_dim'] + self.params['thm_tof_dim'] + self.params['tof_raw_dim'])\n            }\n\n        pred_by_model = {model_type: [] for model_type in models_to_use}\n        \n        if by_fold is None:\n            \n            for key, models in self.models.items():\n                if key in models_to_use:\n                    for model in models:\n                        model.eval()\n                        with torch.no_grad():\n                            output, _ =  model(\n                                torch_seq[:, :, indices_branches['imu']], \n                                torch_seq[:, :, indices_branches['thm_tof']], \n                                torch_seq[:, :, indices_branches['tof_raw']]\n                                )  # [N, num_classes]\n                            probs = F.softmax(output, dim=1).cpu().numpy()  # [N, num_classes]\n                            pred_by_model[key].append(probs)\n                else:\n                    continue\n\n            # Merge predictions\n            N, num_classes = pred_by_model[models_to_use[0]][0].shape\n            merged_probs = np.zeros((N, num_classes))\n\n            for key, pred_list in pred_by_model.items():\n                if not pred_list:\n                    continue\n                stacked = np.stack(pred_list, axis=0)  # [num_models, N, num_classes]\n                avg_probs = np.mean(stacked, axis=0)   # [N, num_classes]\n                merged_probs += weights[key] * avg_probs\n\n            # Final prediction by argmax\n            preds = merged_probs.argmax(axis=1)\n            final_preds = [str(self.map_classes[pred]) for pred in preds]  # [N]\n\n            #     for model in models:\n            #         model.eval()\n            #         with torch.no_grad():\n            #             output = model(torch_seq)  # [N, num_classes]\n            #             preds = output.argmax(1).cpu().numpy()  # shape: [N]\n            #             pred_by_model[key].append(preds)  # list of arrays\n            # pred_by_model = list(zip(*pred_by_model))  # shape: [N, num_models]\n            # final_preds = []\n            # for sample_preds in pred_by_model:\n            #     most_common_prediction = Counter(sample_preds).most_common(1)[0][0]\n            #     final_preds.append(str(self.map_classes[most_common_prediction]))\n        else:      \n            for key, models in self.models.items():\n                if key in models_to_use:\n                    model = models[by_fold]\n                    model.eval()\n                    with torch.no_grad():\n                        output, _ =  model(\n                                torch_seq[:, :, indices_branches['imu']], \n                                torch_seq[:, :, indices_branches['thm_tof']], \n                                torch_seq[:, :, indices_branches['tof_raw']]\n                                )  # [N, num_classes]\n                        probs = F.softmax(output, dim=1).cpu().numpy()  # [N, num_classes]\n                        pred_by_model[key].append(probs)\n                else:\n                    continue\n\n                # Merge predictions\n            print(pred_by_model)\n            N, num_classes = pred_by_model[models_to_use[0]][0].shape\n            merged_probs = np.zeros((N, num_classes))\n\n            for key, pred_list in pred_by_model.items():\n                if not pred_list:\n                    continue\n                stacked = np.stack(pred_list, axis=0)  # [num_models, N, num_classes]\n                avg_probs = np.mean(stacked, axis=0)   # [N, num_classes]\n                merged_probs += weights[key] * avg_probs\n\n                # Final prediction by argmax\n            preds = merged_probs.argmax(axis=1)\n            final_preds = [str(self.map_classes[pred]) for pred in preds]  # [N]\n\n            # else:\n            #     model = self.models['hybrid'][by_fold]\n            #     model.eval()\n            #     with torch.no_grad():\n            #         output = model(torch_seq)  # [N, num_classes]\n            #         preds = output.argmax(1).cpu().numpy()  # shape: [N]\n            #     final_preds = [str(self.map_classes[pred]) for pred in preds]\n        \n        if len(final_preds) == 1:\n            return final_preds[0]\n        else:\n            return final_preds  # length N list of mapped predictions\n\n\n\n        \n   \n\n\n\n \n\n\n# class EnsemblePredictor:\n#     def __init__(self,  processing_dir, models_dir, device):\n#         self.device = device\n#         self.models = []\n#         self.scaler = None\n#         self.features = None\n#         self.label_encoder = None\n#         self.map_classes = None\n#         self.inverse_map_classes = None\n#         self.cols = None\n#         self._load_models(models_dir)\n#         self._load_processing(processing_dir)\n\n#     def _load_models(self, models_dir):\n#         model_files = sorted(glob.glob(f\"{models_dir}/best_model_fold_*.pth\"))\n#         print(f\"{len(model_files)} models have been found\")\n        \n#         for model_file in model_files:\n#             checkpoint = torch.load(model_file, map_location=self.device, weights_only=True)\n            \n#             model = MiniGestureClassifier(imu_dim=14, hidden_dim=128, num_classes=18)\n\n#             model.load_state_dict(checkpoint) #['model_state_dict']\n#             model.to(self.device)\n#             model.eval()\n#             self.models.append(model)\n\n#     def _load_processing(self, processing_dir):\n#         self.scaler = joblib.load(os.path.join(processing_dir, \"scaler.pkl\"))\n#         self.label_encoder = joblib.load(os.path.join(processing_dir, \"label_encoder.pkl\"))\n#         self.map_classes = {idx: cl for idx, cl in enumerate(self.label_encoder.classes_)}\n#         self.inverse_map_classes = {cl: idx for idx, cl in enumerate(self.label_encoder.classes_)}\n\n        \n#         file_path_cols = os.path.join(processing_dir, \"cols.pkl\")\n#         with open(file_path_cols, 'rb') as f:\n#             self.cols = pickle.load(f)\n#         self.features = np.concatenate( (self.cols['imu'], self.cols['thm'], self.cols['tof']) ) \n\n#         print(\"-> scaler, features, labels classes loaded\")\n#         #print(f\"features = {self.features}\")\n    \n#     def features_eng(self, df_seq: pd.DataFrame):\n#         df_seq = regularize_quaternions_per_sequence(df_seq)\n\n#         ### -- ADD NEW FEATURES (IMU + AVERAGED TOF COLUMNS) --- \n#         df_seq = df_seq.reset_index(drop=True)\n#         df_seq = add_gesture_phase(df_seq)\n#         df_seq = compute_acceleration_features(df_seq)\n#         df_seq = compute_angular_features(df_seq)\n#         df_seq = manage_tof(df_seq)\n#         return df_seq\n    \n#     def scale_pad_and_transform_to_torch_sequence(self, df_seq, pad_length, is_imu_only = True):\n#         ### -- Columns re-ordering to match train order\n#         df_seq_features = df_seq[self.features].copy()\n\n#         #has_nan_tof_thm = df_seq_features[ np.concatenate( (self.cols['tof'], self.cols['thm']) ) ].isnull().all(axis=1).all()\n#         # if has_nan_tof_thm:\n#         #     print(\"NaN values have been found in TOF and/or THM data\")\n        \n#         has_nan_imu = df_seq_features[self.cols['imu']].isnull().any().any()\n#         if has_nan_imu:\n#             print(\"x IMU cols have NaN values. Shouldn't be the case! Check data!\")\n\n#         ### -- Scale features and check NaN for IMU COLS  \n#         np_seq_features  =  df_seq_features.to_numpy()\n#         features_to_exclude = [f for f in self.features if any(substr in f for substr in ['phase_adj'])]\n#         features_to_scale = [f for f in self.features if f not in features_to_exclude]\n#         idx_to_scale = np.where(np.isin(self.features, features_to_scale))[0]\n#         if len(np_seq_features) > 0:\n#             np_seq_features[:, idx_to_scale] =  self.scaler.transform(np_seq_features[:, idx_to_scale])\n\n#         if is_imu_only:\n#             imu_features = [\n#             'acc_x','acc_y','acc_z', 'rotvec_x', 'rotvec_y', 'rotvec_z', \n#             'linear_acc_x', 'linear_acc_y', 'linear_acc_z', \n#             'ang_vel_x', 'ang_vel_y', 'ang_vel_z', 'ang_dist',\n#             'phase_adj'\n#             ] \n#             idx_imu = [np.where(self.features == f)[0][0] for f in imu_features]    ### select features from selected_features above\n#             np_seq_features = np_seq_features[:, idx_imu]\n\n\n#         seq = torch.tensor(np_seq_features, dtype=torch.float32)\n#         length = seq.size(0)\n#         # Truncate\n#         if length >= pad_length:\n#             seq = seq[:pad_length].unsqueeze(0)\n#         # Pad\n#         elif length < pad_length:\n#             pad_len = pad_length - length\n#             padding = torch.full((pad_len, *seq.shape[1:]), 0.0, dtype=torch.float32)\n#             seq = torch.cat([seq, padding], dim=0).unsqueeze(0)\n\n#         #print(f\"sequence has been scaled and padded. shape (1, T, F): {seq.shape}\")\n#         return seq.to(self.device)\n    \n#     def predict(self, torch_seq, by_fold = None):\n#     # torch_seq: [N, ...]  (N = batch size)\n\n#         if by_fold is None:\n#             pred_by_model = []\n    \n#             for model in self.models:\n#                 model.eval()\n#                 with torch.no_grad():\n#                     output = model(torch_seq)  # [N, num_classes]\n#                     preds = output.argmax(1).cpu().numpy()  # shape: [N]\n#                     pred_by_model.append(preds)  # list of arrays\n        \n#             # Transpose to get predictions per sample:\n#             # pred_by_model: list of model predictions → shape: [num_models, N]\n#             # after zip(*...), we get: [ [model1_pred_sample1, model2_pred_sample1, ...], ... ]\n#             pred_by_model = list(zip(*pred_by_model))  # shape: [N, num_models]\n        \n#             final_preds = []\n#             for sample_preds in pred_by_model:\n#                 most_common_prediction = Counter(sample_preds).most_common(1)[0][0]\n#                 final_preds.append(str(self.map_classes[most_common_prediction]))\n#         else:\n#             model = self.models[by_fold]\n#             model.eval()\n#             with torch.no_grad():\n#                 output = model(torch_seq)  # [N, num_classes]\n#                 preds = output.argmax(1).cpu().numpy()  # shape: [N]\n#             final_preds = [str(self.map_classes[pred]) for pred in preds]\n        \n#         if len(final_preds) == 1:\n#             return final_preds[0]\n#         else:\n#             return final_preds  # length N list of mapped predictions\n\n\n# class GestureClassifier(nn.Module):\n#     def __init__(self, imu_dim, hidden_dim, num_classes, tof_dim = None, thm_dim = None): # tabular_dim = None\n#         super().__init__()\n\n#         if tof_dim is None:\n#             tof_dim = imu_dim\n#         if thm_dim is None:\n#             thm_dim = imu_dim\n\n#         self.imu_encoder = IMUEncoder(imu_dim, hidden_dim)\n#         self.tof_encoder = OptionalEncoder(tof_dim, hidden_dim)\n#         self.thm_encoder = OptionalEncoder(thm_dim, hidden_dim)\n#         self.fusion = GatedFusion(hidden_dim, num_modalities=3)\n\n\n#         self.classifier_rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n#         self.classifier_head = nn.Sequential(\n#             nn.Linear(hidden_dim * 2 , hidden_dim),\n#             nn.ReLU(),\n#             nn.Dropout(p = 0.3),\n#             nn.Linear(hidden_dim, num_classes),\n#             #nn.Softmax()\n#         )\n\n#     def forward(self, imu, thm=None, tof=None): #, tabular_feats=None\n#         B, T, _ = imu.shape\n\n#         imu_feat = self.imu_encoder(imu)  # [B, hidden_dim, T/4]\n\n#         # nan_mask = torch.isnan(thm)\n#         # nan_indices = torch.nonzero(nan_mask, as_tuple=True)[0].detach().cpu()\n#         # print(f\"number of NaN (detect possible FE errors): {len(np.unique(nan_indices.numpy()))}\")\n\n#         if tof is None:\n#             tof = torch.zeros_like(imu)\n#             tof_mask = torch.zeros(B, T, device=imu.device)\n#         else:\n#             tof_mask = (~torch.isnan(tof).any(dim=2)).float()\n\n#         if thm is None:\n#             thm = torch.zeros_like(imu)\n#             thm_mask = torch.zeros(B, T, device=imu.device)\n#         else:\n#             thm_mask = (~torch.isnan(thm).any(dim=2)).float()\n\n#         tof_feat = self.tof_encoder(tof, tof_mask)  # [B, hidden_dim, T/4]\n#         thm_feat = self.thm_encoder(thm, thm_mask)  # [B, hidden_dim, T/4]\n\n#         fused =  self.fusion([imu_feat, tof_feat, thm_feat])  # [B, hidden_dim, T/4]\n\n#         fused_t = fused.permute(0, 2, 1)  # [B, T/4, hidden_dim]\n#         rnn_out, _ = self.classifier_rnn(fused_t)  # [B, T/4, hidden_dim*2]\n\n#         pooled = rnn_out.mean(dim=1)#   # [B, hidden_dim*2]\n#         #pooled = F.dropout(pooled, p=0.5, training=self.training) \n\n#         out = self.classifier_head(pooled)  # [B, num_classes]\n\n#         return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:26.855281Z","iopub.execute_input":"2025-08-01T13:55:26.855895Z","iopub.status.idle":"2025-08-01T13:55:26.957284Z","shell.execute_reply.started":"2025-08-01T13:55:26.855876Z","shell.execute_reply":"2025-08-01T13:55:26.956675Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"from sklearn.metrics import recall_score\nimport os\n\n\ndef train_model(model, \n                train_loader, val_loader, \n                optimizer, criterion, \n                epochs,\n                device, \n                seed_CV_fold = None,\n                patience = 50, \n                fold = None, \n                logger = None, \n                split_indices = None, \n                scheduler = None, \n                hide_val_half = True,\n                L_IMU = 0.2\n                ):\n    reset_seed(42)\n    model.to(device)\n    early_stopper = EarlyStopping(patience=patience, mode='max', restore_best_weights=True, verbose=True, logger = logger)\n    if split_indices is not None:\n        idx_thm_tof = list(split_indices['thm']) + list(split_indices['tof'])\n    \n    if logger is not None:\n        logger.info(f\"lengths features: \\\n                            {len(split_indices['imu'])} (IMU) \\\n                            {len(idx_thm_tof)} (TOF-THM) \\\n                            {len(split_indices['tof_raw'])} (TOF-RAW) \\\n                            \")\n    else:\n        print(f\"lengths features: \\\n                            {len(split_indices['imu'])} (IMU) \\\n                            {len(idx_thm_tof)} (TOF-THM) \\\n                            {len(split_indices['tof_raw'])} (TOF-RAW) \\\n                            \")\n    best_score = 0\n    best_score_imu_only = 0\n    best_score_imu_tof_thm = 0\n    for epoch in range(1, epochs + 1):\n        #check_memory()\n        model.train()\n        train_loss = 0\n        train_preds = []\n        train_targets = []\n\n        for inputs, targets in tqdm(train_loader, f\"Epoch {epoch}\"):\n\n            # if hide_val_half and split_indices is not None:\n            #     half = batch_size // 2\n            #     x_front = inputs[:half]               \n            #     x_back  = inputs[half:].clone()   \n            #     x_back[:, :, idx_thm_tof] = 0.0    \n            #     inputs = torch.cat([x_front, x_back], dim=0)\n            # print(targets[:5])\n            # print(inputs[0, :10, 0])\n            inputs, targets = inputs.to(device), targets.to(device)\n            #check_memory()\n            optimizer.zero_grad()\n            if split_indices is not None:\n                outputs, imu_logits = model(inputs[:, :, split_indices['imu']], inputs[:, :, idx_thm_tof], inputs[:, :, split_indices['tof_raw']]) #, phase_adj = inputs[:, :,  -1]\n            else:\n                outputs = model(inputs) #, phase_adj = inputs[:, :,  -1]\n            #check_memory()\n            #targets = targets * (1 - 0.1) + (0.1 / 18)\n            imu_loss = criterion(imu_logits, targets)\n            loss = criterion(outputs, targets) #, class_weight, bfrb_classes)\n            loss += L_IMU * imu_loss\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_preds.extend(outputs.argmax(1).cpu().numpy())\n            train_targets.extend(targets.argmax(1).cpu().numpy())\n        \n\n        train_acc, _, train_macro_f1  = competition_metric(train_targets, train_preds)\n\n        # ---- Validation ----\n        model.eval()\n        val_loss = 0\n        val_preds = {'out': [], 'imu_only': [], 'all': []}\n        val_targets = {'out': [], 'imu_only': [], 'all': []}\n        # bin_preds = []\n        # bin_targets = []\n\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                #check_memory()\n                if hide_val_half and split_indices is not None:\n                    B = inputs.shape[0]\n                    half = B // 2\n                    x_front = inputs[:half]               \n                    x_back  = inputs[half:].clone()   \n                    x_back[:, :, len(split_indices['imu']):] = 0.0    \n                    inputs = torch.cat([x_front, x_back], dim=0)\n                    x_back, x_front = x_back.to(device), x_front.to(device)\n\n\n                inputs, targets = inputs.to(device), targets.to(device)\n                if split_indices is not None:\n                    outputs, imu_logits = model(inputs[:, :, split_indices['imu']], inputs[:, :, idx_thm_tof], inputs[:, :, split_indices['tof_raw']]) \n                    assert x_back[:, :, split_indices['imu']].shape[2] > 0, \"IMU split is empty!\"\n                    outputs_imu_only, _ = model(x_back[:, :, split_indices['imu']], x_back[:, :, idx_thm_tof], x_back[:, :, split_indices['tof_raw']]) \n                    outputs_all, _ = model(x_front[:, :, split_indices['imu']], x_front[:, :, idx_thm_tof], x_front[:, :, split_indices['tof_raw']]) \n                else:\n                    outputs = model(inputs) #, phase_adj = inputs[:, :,  -1]               \n                \n                loss = criterion(outputs, targets) #, class_weight, bfrb_classes)\n                imu_loss = criterion(imu_logits, targets) #, class_weight, bfrb_classes)\n                loss += L_IMU * imu_loss\n                val_loss += loss.item()\n\n                if split_indices is not None:\n                    val_preds['all'].extend(outputs_all.argmax(1).cpu().numpy())\n                    val_preds['imu_only'].extend(outputs_imu_only.argmax(1).cpu().numpy())\n\n                    val_targets['all'].extend(targets[:half].argmax(1).cpu().numpy())\n                    val_targets['imu_only'].extend(targets[half:].argmax(1).cpu().numpy())\n\n                val_preds['out'].extend(outputs.argmax(1).cpu().numpy())\n                val_targets['out'].extend(targets.argmax(1).cpu().numpy())\n\n\n                # mask_bfrb_classes = np.array([idx in bfrb_classes.numpy() for idx in range(outputs.shape[1])])\n                # outputs = torch.nn.functional.softmax(outputs, dim=1)\n        \n                # bin_pred = outputs[:, mask_bfrb_classes].sum(1) > 0.4 #torch.stack([, outputs[:, ~mask_bfrb_classes].sum(1)], dim=1) \n                # bin_preds.extend(bin_pred.cpu().numpy())\n\n                # bin_target = targets[:, mask_bfrb_classes].sum(1) #, targets[:, ~mask_bfrb_classes].sum(1)], dim=1) \n                # bin_targets.extend(bin_target.cpu().numpy())\n                \n        val_acc, _, val_macro_f1 = competition_metric(val_targets['out'], val_preds['out'])     #accuracy_score(val_targets, val_preds)\n        early_stopper(val_acc, model)\n        if scheduler is not None:\n            scheduler.step(val_acc)\n\n        #val_binary_recall = recall_score(bin_targets, bin_preds)\n        if early_stopper.best_score > best_score:\n            best_score = early_stopper.best_score\n            name = \"best_model\"\n            if (fold is not None) and (seed_CV_fold is not None):\n                name += f\"_fold_{fold}_seed_{seed_CV_fold}.pth\"\n            else:\n                name += \".pth\"\n            #torch.save(early_stopper.best_model_state, os.path.join(Config.EXPORT_MODELS_PATH, name ))\n\n        \n        \n        if split_indices is not None:\n            val_acc_all, _, _ = competition_metric(val_targets['all'], val_preds['all'])    \n            val_acc_imu_only, _, _ = competition_metric(val_targets['imu_only'], val_preds['imu_only'])   \n            if logger is not None:\n                logger.info(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Macro: {train_macro_f1:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f},  Acc (imu only): {val_acc_imu_only:.4f},  Acc (imu+thm+tof): {val_acc_all:.4f}, Macro: {val_macro_f1:.4f}\")\n            else:\n                print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Macro: {train_macro_f1:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f},  Acc (imu only): {val_acc_imu_only:.4f},   Acc (imu+thm+tof): {val_acc_all:.4f},  Macro: {val_macro_f1:.4f}\")\n        \n            ### BEST IMU-ONLY MODEL ###\n            if  val_acc_imu_only > best_score_imu_only:\n                best_score_imu_only = val_acc_imu_only\n                name = \"best_model_imu_only\"\n                if (fold is not None) and (seed_CV_fold is not None):\n                    name += f\"_fold_{fold}_seed_{seed_CV_fold}.pth\"\n                else:\n                    name += \".pth\"\n                #torch.save(model.state_dict(), os.path.join(Config.EXPORT_MODELS_PATH, name ))\n        \n            ### BEST IMU-TOF-THM MODEL ###\n            if  val_acc_all > best_score_imu_tof_thm:\n                best_score_imu_tof_thm = val_acc_all\n                name = \"best_model_imu_tof_thm\"\n                if (fold is not None) and (seed_CV_fold is not None):\n                    name += f\"_fold_{fold}_seed_{seed_CV_fold}.pth\"\n                else:\n                    name += \".pth\"\n                #torch.save(model.state_dict(), os.path.join(Config.EXPORT_MODELS_PATH, name ))\n        \n        else:\n            if logger is not None:\n                logger.info(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Macro: {train_macro_f1:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Macro: {val_macro_f1:.4f}\")\n            else:\n                print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Macro: {train_macro_f1:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Macro: {val_macro_f1:.4f}\")\n\n        if early_stopper.early_stop:\n            if logger is not None:\n                logger.info(\"Training stopped early.\")\n            else:\n                print(\"Training stopped early.\")\n            break\n\n\n    return best_score, best_score_imu_only, best_score_imu_tof_thm \n\n\n\n        \n\n\n\n\nclass SoftCrossEntropy:\n    def __init__(self,\n                 bfrb_classes = None, gamma = None, lamb = None, class_weights = None, device = DEVICE):      \n        self.gamma = gamma\n        self.lamb = lamb\n        self.class_weights = class_weights\n        self.bfrb_classes = bfrb_classes\n        self.device = device\n\n    def __call__(self,\n                 preds: torch.tensor,\n                 soft_targets: torch.tensor\n                 ):\n        \n        outputs = torch.nn.functional.softmax(preds, dim=1)\n        preds_log = F.log_softmax(preds, dim=1)\n\n\n        if self.class_weights is not None:\n            soft_targets = soft_targets * self.class_weights.to(self.device).unsqueeze(0)\n            soft_targets = soft_targets / soft_targets.sum(dim=1, keepdim=True)  # re-normalize\n\n        weighted_kl = F.kl_div(preds_log, soft_targets, reduction='batchmean')\n\n        if self.bfrb_classes is None and (self.gamma is not None or self.lamb is not None):\n            raise ValueError(\"bfrb_classes should not be None when lamb or gamma is specified\")\n\n        if self.bfrb_classes is not None and (self.gamma is not None or self.lamb is not None):\n            mask_bfrb_classes = np.array([idx in self.bfrb_classes.numpy() for idx in range(preds.shape[1])])\n            \n\n            bfrb_pred = torch.cat( [outputs[:, mask_bfrb_classes], outputs[:, ~mask_bfrb_classes].sum(dim=1, keepdim=True)], dim=1)\n            bfrb_target = torch.cat( [soft_targets[:, mask_bfrb_classes], soft_targets[:, ~mask_bfrb_classes].sum(dim=1, keepdim=True)], dim=1)\n\n            bin_pred = torch.stack([ outputs[:, mask_bfrb_classes].sum(1), outputs[:, ~mask_bfrb_classes].sum(1)], dim=1)\n            bin_target = torch.stack([soft_targets[:, mask_bfrb_classes].sum(1), soft_targets[:, ~mask_bfrb_classes].sum(1)], dim=1) \n\n            brfb_loss = F.kl_div(\n            torch.log(bfrb_pred + 1e-8),  # log-probabilities\n            bfrb_target,\n            reduction='batchmean'\n            )\n\n            binary_loss = F.kl_div(\n            torch.log(bin_pred + 1e-8),  # log-probabilities #torch.log(+1e-8)\n            bin_target,\n            reduction='batchmean'\n            )\n\n\n            if self.gamma is not None and self.lamb is None:\n                return  weighted_kl + self.gamma * brfb_loss \n            if self.gamma is None and self.lamb is not None:\n                return  weighted_kl + self.lamb * binary_loss \n            if self.gamma is not None and self.lamb is not None:\n                return   weighted_kl + self.gamma * brfb_loss + self.lamb * binary_loss    \n        else:\n            return weighted_kl\n\n\ndef predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    sequence = sequence.to_pandas()\n    demographics = demographics.to_pandas()\n    sequence = predictor.features_eng(sequence, demographics)\n    torch_seq = predictor.scale_pad_and_transform_to_torch_sequence(sequence, pad_length)\n    most_common_prediction = predictor.predict(torch_seq)\n    return str(most_common_prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:26.958422Z","iopub.execute_input":"2025-08-01T13:55:26.958587Z","iopub.status.idle":"2025-08-01T13:55:26.983838Z","shell.execute_reply.started":"2025-08-01T13:55:26.958574Z","shell.execute_reply":"2025-08-01T13:55:26.983138Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"\"\"\"\nHierarchical macro F1 metric for the CMI 2025 Challenge.\n\nThis script defines a single entry point `score(solution, submission, row_id_column_name)`\nthat the Kaggle metrics orchestrator will call.\nIt performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.metrics import f1_score\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n    pass\n\n\nclass CompetitionMetric:\n    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n    def __init__(self):\n        self.target_gestures = [\n            'Above ear - pull hair',\n            'Cheek - pinch skin',\n            'Eyebrow - pull hair',\n            'Eyelash - pull hair',\n            'Forehead - pull hairline',\n            'Forehead - scratch',\n            'Neck - pinch skin',\n            'Neck - scratch',\n        ]\n        self.non_target_gestures = [\n            'Write name on leg',\n            'Wave hello',\n            'Glasses on/off',\n            'Text on phone',\n            'Write name in air',\n            'Feel around in tray and pull out an object',\n            'Scratch knee/leg skin',\n            'Pull air toward your face',\n            'Drink from bottle/cup',\n            'Pinch knee/leg skin'\n        ]\n        self.all_classes = self.target_gestures + self.non_target_gestures\n\n    def calculate_hierarchical_f1(\n        self,\n        sol: pd.DataFrame,\n        sub: pd.DataFrame\n    ) -> float:\n\n        # Validate gestures\n        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n        if invalid_types:\n            raise ParticipantVisibleError(\n                f\"Invalid gesture values in submission: {invalid_types}\"\n            )\n\n        # Compute binary F1 (Target vs Non-Target)\n        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n        f1_binary = f1_score(\n            y_true_bin,\n            y_pred_bin,\n            pos_label=True,\n            zero_division=0,\n            average='binary'\n        )\n\n        # Build multi-class labels for gestures\n        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n\n        # Compute macro F1 over all gesture classes\n        f1_macro = f1_score(\n            y_true_mc,\n            y_pred_mc,\n            average='macro',\n            zero_division=0\n        )\n\n        return 0.5 * f1_binary + 0.5 * f1_macro\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str\n) -> float:\n    \"\"\"\n    Compute hierarchical macro F1 for the CMI 2025 challenge.\n\n    Expected input:\n      - solution and submission as pandas.DataFrame\n      - Column 'sequence_id': unique identifier for each sequence\n      - 'gesture': one of the eight target gestures or \"Non-Target\"\n\n    This metric averages:\n    1. Binary F1 on SequenceType (Target vs Non-Target)\n    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n\n    Raises ParticipantVisibleError for invalid submissions,\n    including invalid SequenceType or gesture values.\n\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> row_id_column_name = \"id\"\n    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n    0.5\n    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n    0.0\n    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n    1.0\n    \"\"\"\n    # Validate required columns\n    for col in (row_id_column_name, 'gesture'):\n        if col not in solution.columns:\n            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n        if col not in submission.columns:\n            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n\n    metric = CompetitionMetric()\n    return metric.calculate_hierarchical_f1(solution, submission)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:26.984655Z","iopub.execute_input":"2025-08-01T13:55:26.984954Z","iopub.status.idle":"2025-08-01T13:55:27.001162Z","shell.execute_reply.started":"2025-08-01T13:55:26.984937Z","shell.execute_reply":"2025-08-01T13:55:27.000410Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import WeightedRandomSampler\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport sys\n\nprint(f\"✓ Configuration loaded for Kaggle environment (Device: {DEVICE})\")\nprint(\"Device in use:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n\nif torch.cuda.is_available():\n    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n    print(f\"Using device: {device_name}\")\n\n\nTRAIN = False\n\ndata_file =  \"train_torch_tensors_from_wrapper_left_corrected_without_TOF_correction.pt\"\n\n\nN_SPLITS = 5\nBATCH_SIZE = 64\nEPOCHS = 160\nHIDDEN_DIM = 128\nPATIENCE = 45\nALPHA = 0.4\nLR = 1e-3\nSEED_CV_FOLD = 39\n\np_dropout = 0.48 #0.42\np_jitter= 0.0 #0.98\np_moda = 0.4 #0.4\np_rotation = 1.1\nsmall_rotation = 2.\nx_max_angle = 30.\ny_max_angle = 15.\n\nnormalisation_TOF_RAW = False\nnormalisation_TOF_THM = True\nattention_for_fusion = False\nattention_pooled = True\nC_TOF_RAW = False\nADD_TOF_TO_THM = True\n\nSCHEDULER = True\npatience_scheduler = 8\nfactor_scheduler = 0.7\n\n\nGAMMA = 0.0\nLAMB = 0.0\nL_IMU = 0.25\n\n\nSEED = Config.SEED\nreset_seed(SEED)\n\nfile_path_train = os.path.join(Config.EXPORT_DIR, data_file)\nfile_path_cols = os.path.join(Config.EXPORT_DIR, \"cols.pkl\")\nfile_path_splits = os.path.join(Config.EXPORT_DIR, \"split_ids.pkl\")\n\n\nselected_features = [\n        'acc_x','acc_y','acc_z',#,'rot_x', 'rot_y', 'rot_z', 'rot_w', \n        'rotvec_x', 'rotvec_y', 'rotvec_z', \n        'linear_acc_x', 'linear_acc_y', 'linear_acc_z', \n        #'linear_acc_x_FFT', 'linear_acc_y_FFT', 'linear_acc_z_FFT', \n        #'acc_norm_world', \n        # 'acc_norm', 'linear_acc_norm', \n        # 'acc_norm_jerk', 'linear_acc_norm_jerk', \n        #'angle_rad', 'angular_speed', \n        # 'rot_angle', 'rot_angle_vel', 'angular_speed', \n        'ang_vel_x', 'ang_vel_y', 'ang_vel_z', 'ang_dist',\n        # 'ang_vel_x_FFT', 'ang_vel_y_FFT', 'ang_vel_z_FFT', \n        'phase_adj',\n        ] \n\nprint(\"Features:\", selected_features)\n\n\n# ---------------- LOAD DATA ------------------------\n\n\nif os.path.exists(file_path_train):\n    print(\"Loading existing tensor...\")\n    data = torch.load(file_path_train)\n    X_train, y_train = data['X_train'], data['y_train']\n\n    with open(file_path_cols, 'rb') as f:\n        cols = pickle.load(f)\n\n    with open(file_path_splits, 'rb') as f:\n        split_ids = pickle.load(f)\n\n\nelse:\n    print(\"File not found. Generating data...\")\n    X_train, y_train = wrapper_data(split=False)\n    print(X_train.shape, y_train.shape)\n\n    data = {'X_train': X_train, 'y_train': y_train} \n    torch.save(data, file_path_train)\n    print(\"Data saved.\")\n\n    with open(file_path_cols, 'rb') as f:\n        cols = pickle.load(f)\n\n    with open(file_path_splits, 'rb') as f:\n        split_ids = pickle.load(f)\n\n\ngesture_mapping = {cl: idx for idx, cl in split_ids['classes'].items()}   ### GESTURE MAP CLASSES --> LABELS\nbfrb_gesture = CompetitionMetric().target_gestures                        ### TARGET GESTURE CLASSES\nbfrb_classes = torch.tensor([gesture_mapping[cl] for cl in bfrb_gesture]) ### TARGET GESTURE LABELS\n\n\n# ------------------ SELECT FEATURES AND PREPARE DATA FOR TRAINING ------------------------\n\nall_features = np.concatenate( (cols['imu'], cols['thm'], cols['tof']) ) \nselected_tof = [f for f in cols['tof'] if ('v' not in f) and ('tof_5' not in f)]\nraw_tof = [f for f in cols['tof'] if ('v' in f) and ('tof_5' not in f)]\nprint(raw_tof)\n\nraw_tof_sorted = np.array([f'tof_{i}_v{j}' for i in range(1, 5) for j in range(64)])\ncheck_all_pixels = np.array([f in raw_tof for f in raw_tof_sorted]   )            ### THM Features for later\n\nif not np.all(check_all_pixels):\n    print(f\"missing pixel raw data in TOF data: {np.array(raw_tof_sorted)[~check_all_pixels]}\")\n\n\n\nraw_tof_sorted = list(raw_tof_sorted[check_all_pixels])\n\n\nidx_imu = [np.where(all_features == f)[0][0] for f in selected_features]    ### select features from selected_features above\nidx_tof = [np.where(all_features == f)[0][0] for f in selected_tof]                   ### TOF Features for later\nidx_raw_tof = [np.where(all_features == f)[0][0] for f in raw_tof_sorted]                   ### TOF Features for later\nidx_thm = [np.where(all_features == f)[0][0] for f in cols['thm'] if 'thm_5' not in f]               ### THM Features for later              ### THM Features for later\n\nidx_all = idx_imu + idx_thm + idx_tof + idx_raw_tof\nindices_branches = {\n    'imu': np.arange(len(idx_imu)), \n    'thm': np.arange(len(idx_imu), len(idx_imu + idx_thm)), \n    'tof': np.arange(len(idx_imu + idx_thm), len(idx_imu + idx_thm + idx_tof)),\n    'tof_raw': np.arange(len(idx_imu + idx_thm + idx_tof), len(idx_all))\n    }\n# else:\n#     idx_all = idx_imu + idx_thm + idx_raw_tof\n#     indices_branches = {\n#         'imu': np.arange(len(idx_imu)), \n#         'thm': np.arange(len(idx_imu), len(idx_imu + idx_thm)), \n#         'tof': [],\n#         'tof_raw': np.arange(len(idx_imu + idx_thm), len(idx_all))\n#         }\n\n\nX = X_train[:, :, idx_all]   ## select idx features in X\ny = y_train                  ## labels \n\n\n#### NaN ? in DATA #### \nnan_mask = torch.isnan(X[:, :, indices_branches['imu']])\nnan_indices = torch.nonzero(nan_mask, as_tuple=True)\nprint(f\"number of NaN (detect possible FE errors): {len(np.unique(nan_indices[0].numpy()))}\")\n      \nif len(np.unique(nan_indices[0].numpy())) > 0:      \n    X[:, :, indices_branches['imu']] = torch.tensor(np.nan_to_num(X[:, :, indices_branches['imu']], nan=0.0))\n\nnan_mask = torch.isnan(X)\nnan_indices = torch.nonzero(nan_mask, as_tuple=True)\n      \nif len(np.unique(nan_indices[0].numpy())) > 0:      \n    X = torch.tensor(np.nan_to_num(X, nan=0.0))\n\n########################\n\n\nprint(f\"Data shape (X, y): {X.shape, y.shape}\")\n\n# cw_vals = compute_class_weight('balanced', classes=list(split_ids['classes'].keys()), y=y.numpy())  ## Class weights to handle imbalance\n# class_weight = torch.from_numpy(cw_vals).float()                                                    ## class weights as torch tensor\n\nclass_weight = 0.7 * torch.ones(len(split_ids['classes'].keys()))\nclass_weight[bfrb_classes] = 2.\n\n\n# ----------- ALL PARAMETERS TO SAVE IT ---------------\nif TRAIN:\n    all_parameters = {\n        \"data_file\": data_file,\n        \"SEED\": SEED,\n        \"SEED_CV_FOLD\": SEED_CV_FOLD if SEED_CV_FOLD is not None else None,\n        \"N_SPLITS\": N_SPLITS,\n        \"BATCH_SIZE\": BATCH_SIZE,\n        \"EPOCHS\": EPOCHS,\n        \"HIDDEN_DIM\": HIDDEN_DIM,\n        \"PATIENCE\": PATIENCE,\n        \"ALPHA\":ALPHA,\n        \"LR\": LR,\n        \"normalisation_TOF_RAW\": normalisation_TOF_RAW,\n        \"normalisation_TOF_THM\": normalisation_TOF_THM,\n        \"attention_for_fusion\": attention_for_fusion,\n        \"attention_pooled\": attention_pooled,\n        \"add_tof_features_to_thm\": ADD_TOF_TO_THM,\n        \"C_TOF_RAW\": C_TOF_RAW,\n        \"IMU_FEATURES\": selected_features,\n        \"THM-TOF FEATURES\": selected_tof,\n        \"TOF-RAW FEATURES\": raw_tof_sorted,\n        \"loss_GAMMA\": GAMMA,\n        \"loss_LAMBDA\": LAMB,\n        \"additionnal_IMU_loss\": L_IMU,\n        \"N_CLASSES\": len(class_weight),\n        \"imu_dim\":len(selected_features),\n        \"thm_tof_dim\":len(selected_tof),\n        \"tof_raw_dim\":len(raw_tof_sorted),\n        \"scheduler\": SCHEDULER if SCHEDULER else None,\n        \"factor_scheduler\": factor_scheduler if SCHEDULER else None,\n        \"patience_scheduler\": patience_scheduler if SCHEDULER else None,\n        \"p_dropout\": p_dropout,\n        \"p_jitter\": p_jitter,\n        \"p_moda\": p_moda,\n        \"p_rotation\": p_rotation,\n        \"small_rotation\": small_rotation, \n        \"x_max_angle\": x_max_angle,\n        \"y_max_angle\": y_max_angle,\n    }\nelse:\n    file_path_params = os.path.join(Config.EXPORT_DIR, 'all_parameters.pkl')\n    with open(file_path_params, 'rb') as f:\n        file_params = pickle.load(f)\n    all_parameters = file_params['hyperparams']\n    all_parameters[\"thm_tof_dim\"] = 20\n    print(all_parameters)\n\n# ------------------------------- DEMO DATA ---------------------------------\n \ntrain_demographics = pd.read_csv(Config.TRAIN_DEMOGRAPHICS_PATH)\n\n# ------------------------------- TRAINING ---------------------------------\n\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state = SEED_CV_FOLD) #STRATIFIED k-Fold by group (subject_id)\n\nif not ADD_TOF_TO_THM:\n    indices_branches['tof'] = []\n\ntrain_ids = np.array(split_ids['train']['train_sequence_ids']) #seq_id of data sequences \ngroups = [split_ids['train']['train_sequence_subject'][seq_id] for seq_id in train_ids] #subject_id of data_sequences\nwrong_subjects = ['SUBJ_045235', 'SUBJ_019262']\n\n# idx_spe_seq = np.where(train_ids == 'SEQ_000007')[0]\n\n\n### LOOP FOR EACH TRAINING FOLD\nbest_scores = {\n    'mixture':[],\n    'imu_only':[],\n    'imu_tof_thm':[], \n    }\nbest_scores_inference = []\nfor fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y, groups)):\n    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\\n\")\n    reset_seed(SEED)\n\n    # Split data\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    if TRAIN:\n\n        subjects_id = np.array(groups)[train_idx]\n        train_seq_ids = train_ids[train_idx]\n\n        ###### Handedness of subjects in train and validation fold\n        subjects_fold_train = np.unique(subjects_id)\n        subjects_fold_val= np.unique(np.array(groups)[val_idx])\n        handedness_train = [train_demographics[train_demographics['subject'] == subject]['handedness'].iloc[0] for subject in subjects_fold_train]\n        handedness_val = [train_demographics[train_demographics['subject'] == subject]['handedness'].iloc[0] for subject in subjects_fold_val]\n        print(f\"number of left-handed (right-handed) subject in train fold {fold + 1} = {np.sum(np.array(handedness_train) == 0)} ({np.sum(np.array(handedness_train) == 1)})\")\n        print(f\"number of left-handed (right-handed) subject in val fold {fold + 1} = {np.sum(np.array(handedness_val) == 0)} ({np.sum(np.array(handedness_val) == 1)})\")\n        cond_wrong = [wg_sub in subjects_fold_val for wg_sub in wrong_subjects]\n        if any(cond_wrong):\n            print(f\"wrong wrist wearing detected in val fold {fold + 1}: {np.array(wrong_subjects)[cond_wrong]}\")\n\n        print(\" ---- check for reproductibility ----\")\n        print(f\"first 10 seq_id = {train_seq_ids[:10]}\")\n        print(f\"first 10 train idx = {train_idx[:10]}, and val idx = {val_idx[:10]}\")\n        print(f\"mean train idx = {np.mean(train_idx)}, and mean val idx = {np.mean(val_idx)}\\n\")\n\n        df = pd.DataFrame({'subject_id': subjects_id, 'seq_id': train_seq_ids})\n        seqs_by_subject = (\n                df.groupby('subject_id')['seq_id']\n                .unique()\n                .apply(list)\n                .to_dict()\n            )\n\n        #### DATA AUGMENTATION #####\n        print(\"------ DATA AUGMENTATION: DEVICE ROTATION ------\")\n        rotation_augmented = DeviceRotationAugment(X_tr, y_tr, train_seq_ids,     \n                            seqs_by_subject, selected_features, \n                            p_rotation=1.1, \n                            small_rotation=2., \n                            x_rot_range=(0., 30.)\n                            )\n        X_tr, y_tr, count = rotation_augmented(axes=['z', 'x'])\n        print(f\"number of additional rotated features samples: {count}\")\n        print(f\"shape of training data after augmentation (X, y): {X_tr.shape, y_tr.shape}\\n\")\n\n        # augmenter = Augment()\n\n        # augmenter = Augment(\n        #     p_jitter=0.98, sigma=0.033, scale_range=(0.75,1.16),\n        #     p_dropout=0.42,\n        #     p_moda=0.39, drift_std=0.004, drift_max=0.39    \n        # )\n        augmenter = Augment(\n            p_jitter=0.98, sigma=0.033, scale_range=(0.75,1.16),\n            p_dropout=0.42,\n            p_moda=0.39, drift_std=0.004, drift_max=0.39    \n        )\n        #########################################\n\n        train_ds = SensorDataset(X_tr, y_tr, imu_dim = len(idx_imu), alpha=ALPHA, augment=augmenter)  ### TRAINING ROTATION AUGMENTED DATA WITH MixUp \\alpha \n\n\n        # CLASS IMBALANCE handling \n        print(\" ----------- CLASS INBALANCE SAMPLER (WeightedRandomSampler) ---------\") \n        class_counts = np.bincount(y_tr.numpy())\n        print(f\"Number of samples per class: {Counter(y_tr.numpy())}\\n\")\n        class_weights_balanced = 1. / class_counts\n        sample_weights = class_weights_balanced[y_tr.numpy()]\n        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights) , replacement=True)\n        tracking_sampler = TrackingSampler(sampler)\n\n        sampled_indices = list(sampler)\n        sampled_labels = y_tr[sampled_indices]\n        print(Counter(sampled_labels.numpy()))\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=tracking_sampler, pin_memory=True)\n    \n        val_ds = SensorDataset(X_val, y_val, imu_dim = 7, training=False) ### VALIDATION DATA (NO AUG, NO MixUp)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n\n\n    if TRAIN:\n        criterion = SoftCrossEntropy(bfrb_classes=bfrb_classes, gamma = GAMMA, lamb = LAMB) # LOSS FUNCTION bfrb_classes=bfrb_classes, gamma = .5, lamb = .5 \n\n        #model = MiniGestureClassifier(imu_dim=X_tr.shape[2], hidden_dim=128, num_classes=len(class_weight)) # MODEL\n        model = GlobalGestureClassifier(imu_dim=len(indices_branches['imu']), \n                                        thm_tof_dim=len(indices_branches['thm']) + len(indices_branches['tof']), \n                                        tof_raw_dim=len(indices_branches['tof_raw']),  \n                                        hidden_dim=HIDDEN_DIM, \n                                        num_classes=len(class_weight), \n                                        C_TOF_RAW=C_TOF_RAW,\n                                        norm_TOF_RAW=normalisation_TOF_RAW,\n                                        norm_TOF_THM=normalisation_TOF_THM,\n                                        attention_for_fusion=attention_for_fusion,\n                                        attention_pooled= attention_pooled\n                                        ) # MODEL\n\n\n        optimizer = optim.Adam(model.parameters(), lr=LR) # OPTIMIZER  weight_decay=WD\n\n        if SCHEDULER:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=factor_scheduler, patience=patience_scheduler, verbose=True)\n        else:\n            scheduler = None\n        \n        best_score, best_score_imu_only, best_score_imu_tof_thm = train_model(model, train_loader, val_loader, \n                                 optimizer, criterion, \n                                 EPOCHS, \n                                 DEVICE, \n                                 patience=PATIENCE, \n                                 fold = fold, \n                                 split_indices = indices_branches,\n                                 scheduler = scheduler,                                            \n                                 L_IMU= L_IMU,\n                                 seed_CV_fold = SEED_CV_FOLD                                            \n                                 )\n        best_scores['mixture'].append(best_score)\n        best_scores['imu_only'].append(best_score_imu_only)\n        best_scores['imu_tof_thm'].append(best_score_imu_tof_thm)\n    else:\n        print(\"---- INFERENCE MODE ----\")\n        processing_dir = Config.EXPORT_DIR\n        models_dir = Config.EXPORT_MODELS_PATH\n        predictor = EnsemblePredictor(processing_dir, models_dir, DEVICE, all_parameters)\n        inverse_map_classes = predictor.inverse_map_classes\n        #map_classes = predictor.map_classes\n\n        #val_ds = SensorDataset(X_val, y_val, imu_dim = 7, training=False) ### VALIDATION DATA (NO AUG, NO MixUp)\n        #val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n\n        #X_val = []\n        #for inputs, targets in val_loader:\n        #    B = inputs.shape[0]\n        #    half = B // 2\n        #    x_front = inputs[:half]               \n        #    x_back  = inputs[half:].clone()   \n        #    x_back[:, :, 14:] = 0.0    \n        #    inputs = torch.cat([x_front, x_back], dim=0)\n        #    X_val.extend(inputs)\n        #X_val = torch.stack(X_val)\n        \n        preds_str = predictor.predict(X_val.to(DEVICE), by_fold = fold) #, models_to_use = ['hybrid_models'])\n        preds_int = [inverse_map_classes[pred_str] for pred_str in preds_str]\n        best_score, _, _ = competition_metric(y_val, preds_int)\n        print(best_score)\n    \n        best_scores_inference.append(best_score)\n\nif TRAIN:\n    print(np.mean(best_scores['mixture']))\n    print(np.mean(best_scores['imu_only']))\n    print(np.mean(best_scores['imu_tof_thm']))\n\nelse:\n    print(np.mean(best_scores_inference))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:27.002707Z","iopub.execute_input":"2025-08-01T13:55:27.002905Z","iopub.status.idle":"2025-08-01T13:55:35.998434Z","shell.execute_reply.started":"2025-08-01T13:55:27.002889Z","shell.execute_reply":"2025-08-01T13:55:35.997559Z"}},"outputs":[{"name":"stdout","text":"✓ Configuration loaded for Kaggle environment (Device: cuda)\nDevice in use: 0\nUsing device: Tesla T4\nFeatures: ['acc_x', 'acc_y', 'acc_z', 'rotvec_x', 'rotvec_y', 'rotvec_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'ang_vel_x', 'ang_vel_y', 'ang_vel_z', 'ang_dist', 'phase_adj']\nLoading existing tensor...\n['tof_1_v0', 'tof_1_v1', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v2', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v3', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v4', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v5', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v6', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_2_v0', 'tof_2_v1', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v2', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v3', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v4', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v5', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v6', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_3_v0', 'tof_3_v1', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v2', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v3', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v4', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v5', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v6', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_4_v0', 'tof_4_v1', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v2', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v3', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v4', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v5', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v6', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9']\nnumber of NaN (detect possible FE errors): 0\nData shape (X, y): (torch.Size([8151, 127, 290]), torch.Size([8151]))\n{'data_file': 'train_torch_tensors_from_wrapper_left_corrected_without_TOF_correction.pt', 'SEED': 42, 'SEED_CV_FOLD': 39, 'N_SPLITS': 5, 'BATCH_SIZE': 64, 'EPOCHS': 160, 'HIDDEN_DIM': 128, 'PATIENCE': 45, 'ALPHA': 0.4, 'LR': 0.001, 'normalisation_TOF_RAW': False, 'normalisation_TOF_THM': True, 'attention_for_fusion': False, 'attention_pooled': True, 'add_tof_features_to_thm': True, 'C_TOF_RAW': False, 'IMU_FEATURES': ['acc_x', 'acc_y', 'acc_z', 'rotvec_x', 'rotvec_y', 'rotvec_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'ang_vel_x', 'ang_vel_y', 'ang_vel_z', 'ang_dist', 'phase_adj'], 'THM-TOF FEATURES': ['tof_1_max', 'tof_1_mean', 'tof_1_min', 'tof_1_std', 'tof_2_max', 'tof_2_mean', 'tof_2_min', 'tof_2_std', 'tof_3_max', 'tof_3_mean', 'tof_3_min', 'tof_3_std', 'tof_4_max', 'tof_4_mean', 'tof_4_min', 'tof_4_std', 'thm_1', 'thm_2', 'thm_3', 'thm_4'], 'TOF-RAW FEATURES': ['tof_1_v0', 'tof_1_v1', 'tof_1_v2', 'tof_1_v3', 'tof_1_v4', 'tof_1_v5', 'tof_1_v6', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63', 'tof_2_v0', 'tof_2_v1', 'tof_2_v2', 'tof_2_v3', 'tof_2_v4', 'tof_2_v5', 'tof_2_v6', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63', 'tof_3_v0', 'tof_3_v1', 'tof_3_v2', 'tof_3_v3', 'tof_3_v4', 'tof_3_v5', 'tof_3_v6', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63', 'tof_4_v0', 'tof_4_v1', 'tof_4_v2', 'tof_4_v3', 'tof_4_v4', 'tof_4_v5', 'tof_4_v6', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63'], 'loss_GAMMA': 0.0, 'loss_LAMBDA': 0.0, 'additionnal_IMU_loss': 0.25, 'N_CLASSES': 18, 'imu_dim': 14, 'thm_tof_dim': 20, 'tof_raw_dim': 256, 'scheduler': True, 'factor_scheduler': 0.7, 'patience_scheduler': 8, 'p_dropout': 0.48, 'p_jitter': 0.2, 'p_moda': 0.2, 'p_rotation': 1.1, 'small_rotation': 2.0, 'x_max_angle': 30.0, 'y_max_angle': 15.0, 'axes_rotation': ['z', 'x', 'y']}\n\n===== FOLD 1/5 =====\n\n---- INFERENCE MODE ----\n1 hybrid models have been found\n1 imu only models have been found\n1 imu tof thm models have been found\n-> scaler, features, labels classes loaded\n{'hybrid_models': [array([[9.2736905e-04, 9.4114292e-01, 1.7987248e-04, ..., 1.2890669e-04,\n        6.0460350e-04, 4.8441536e-04],\n       [1.0299776e-03, 1.2743184e-03, 4.4070461e-05, ..., 9.5755153e-04,\n        1.0176891e-04, 5.2793945e-05],\n       [9.6493008e-05, 2.4765989e-04, 1.5501781e-04, ..., 2.0483507e-05,\n        5.1261708e-05, 2.3393079e-05],\n       ...,\n       [1.1987492e-04, 7.3518831e-04, 5.0167415e-05, ..., 1.1700938e-04,\n        2.2650211e-05, 1.6168097e-04],\n       [1.0360916e-03, 9.0201408e-01, 3.1370358e-04, ..., 2.4986983e-04,\n        2.1047459e-03, 1.8396083e-03],\n       [3.8480692e-04, 3.5938108e-04, 3.0895185e-03, ..., 3.0184952e-03,\n        6.6575129e-03, 4.1889111e-04]], dtype=float32)], 'imu_only_models': [array([[9.2736905e-04, 9.4114292e-01, 1.7987248e-04, ..., 1.2890669e-04,\n        6.0460350e-04, 4.8441536e-04],\n       [1.0299776e-03, 1.2743184e-03, 4.4070461e-05, ..., 9.5755153e-04,\n        1.0176891e-04, 5.2793945e-05],\n       [9.6493008e-05, 2.4765989e-04, 1.5501781e-04, ..., 2.0483507e-05,\n        5.1261708e-05, 2.3393079e-05],\n       ...,\n       [1.1987492e-04, 7.3518831e-04, 5.0167415e-05, ..., 1.1700938e-04,\n        2.2650211e-05, 1.6168097e-04],\n       [1.0360916e-03, 9.0201408e-01, 3.1370358e-04, ..., 2.4986983e-04,\n        2.1047459e-03, 1.8396083e-03],\n       [3.8480692e-04, 3.5938108e-04, 3.0895185e-03, ..., 3.0184952e-03,\n        6.6575129e-03, 4.1889111e-04]], dtype=float32)], 'imu_tof_thm_models': [array([[1.48621737e-03, 9.06377196e-01, 2.27555909e-04, ...,\n        1.82054602e-04, 6.14872144e-04, 6.05673529e-04],\n       [1.30223285e-03, 8.68258707e-04, 4.37690323e-05, ...,\n        6.08417147e-04, 8.94652621e-05, 5.31108562e-05],\n       [4.94475935e-05, 5.58109932e-05, 1.45795711e-04, ...,\n        1.47780502e-05, 4.28067506e-05, 3.19329483e-05],\n       ...,\n       [7.80865521e-05, 5.08905272e-04, 3.93410337e-05, ...,\n        8.22786460e-05, 1.32893483e-05, 1.06723724e-04],\n       [1.71295169e-03, 8.66305351e-01, 4.39492753e-04, ...,\n        4.29755164e-04, 3.31800198e-03, 2.51521682e-03],\n       [5.03866875e-04, 3.11945070e-04, 3.39596299e-03, ...,\n        4.87820385e-03, 1.35098640e-02, 8.16480082e-04]], dtype=float32)]}\n0.847604615195356\n\n===== FOLD 2/5 =====\n\n---- INFERENCE MODE ----\n1 hybrid models have been found\n1 imu only models have been found\n1 imu tof thm models have been found\n-> scaler, features, labels classes loaded\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1939549626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m#X_val = torch.stack(X_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mpreds_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, models_to_use = ['hybrid_models'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mpreds_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minverse_map_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_str\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompetition_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3196813136.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, torch_seq, by_fold, models_to_use)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_to_use\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mby_fold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":135},{"cell_type":"code","source":"dummy_test = False\n\nif dummy_test:\n\n    pad_length = Config.PADDING\n    processing_dir = Config.EXPORT_DIR\n    models_dir = Config.EXPORT_MODELS_PATH\n    test_path = Config.TEST_PATH\n    train_path = Config.TRAIN_PATH\n    train_path_demo = Config.TRAIN_DEMOGRAPHICS_PATH\n    \n    # Check GPU availability\n    DEVICE = torch.device(check_gpu_availability())\n    print(f\"✓ Configuration loaded for Kaggle environment (Device: {DEVICE})\")\n    \n    \n    predictor = EnsemblePredictor(processing_dir, models_dir, DEVICE, all_parameters)\n    inverse_map_classes = predictor.inverse_map_classes\n    map_classes = predictor.map_classes\n\n    train = pd.read_csv(train_path)\n    train_demo = pd.read_csv(train_path_demo)\n    \n    print(f\"---> Original shape = {train.shape}\")\n    sel_seq  = train[\"sequence_id\"].unique()#[0 : 3500]\n    seq      = sel_seq[0: 1750]\n    oth_cols = sorted([c for c in train.columns if (c.startswith('thm_') or c.startswith('tof_'))]) #train.columns[16:]\n    train    = train.loc[train.sequence_id.isin(sel_seq)]\n    train.loc[train.sequence_id.isin(seq), oth_cols] = np.nan\n    print(f\"---> Truncated shape = {train.shape}\")\n    train_sequences = train.groupby(\"sequence_id\")\n    \n    ypred = []\n    ytruth = []\n    for _, sequence in tqdm(train_sequences, desc=\"Processing Sequences\"):\n    #     #print(f\"======== SEQUENCE {seq_id} ========\")\n        sequence = pl.DataFrame(sequence)\n        pred = predict(sequence, train_demo)\n        ypred.append(inverse_map_classes[pred])\n        sequence = sequence.to_pandas()\n        ytruth.append(inverse_map_classes[sequence['gesture'].iloc[0]])\n    \n    \n    print(competition_metric(ytruth, ypred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:35.998896Z","iopub.status.idle":"2025-08-01T13:55:35.999164Z","shell.execute_reply.started":"2025-08-01T13:55:35.999013Z","shell.execute_reply":"2025-08-01T13:55:35.999024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### SUBMISSION ####\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:55:36.000330Z","iopub.status.idle":"2025-08-01T13:55:36.000615Z","shell.execute_reply.started":"2025-08-01T13:55:36.000499Z","shell.execute_reply":"2025-08-01T13:55:36.000511Z"}},"outputs":[],"execution_count":null}]}